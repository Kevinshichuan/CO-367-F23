\subsection{Lecture 7}
\begin{definition}[Nonlinear Least Square]
  Suppose we have $F: \mathbb R^n \to \mathbb R^m$ where $$F(x) = \begin{pmatrix}
    f_1(x) \\ f_2(x) \\ \vdots \\ f_m(x)
  \end{pmatrix}$$ Then the nonlinear least squares problem is $$\min \{h(x)\}$$ where $$h(x) = \frac{1}{2} \|F(x)\|^2 = \frac{1}{2}\langle F(x), F(x) \rangle = \frac{1}{2} \sum^m_{i=1} f_i^2(x)$$
\end{definition}
\begin{definition}[Jacobian Matrix]
  Let $F$ be defined as above, then the Jacobian matrix is $J(x) = F'(x)$ where $$F'(x) = \begin{bmatrix}
    \nabla f_1(x)^T \\ \nabla f_2(x)^T \\ \vdots \\ \nabla f_m(x)^T
  \end{bmatrix}$$
\end{definition}
\begin{problem}[Solving Nonlinear Least Squares]
  For the nonlinear least squares problem defined above, we consider the special case $m = n$. Then we can consider the problem as solving the square system of nonlinear equations $F(x) = 0$. Recall that for current approximation $x_c$, $$0 = F(x_c + d) \approx F(x_c) + \underbrace{F'(x_c)}_{\text{Jacobian}}\underbrace{d}_{\text{search direction}}$$ So we solve $$F'(x_c)d = -F(x_c)$$ which is called the Newton equation. Then we can take a step in the search/Newton direction $d$ to get a new approxiamtion $x_{c+1} = x_c + \alpha d$ for appropriate step length $\alpha$.
  \\ In general, $\nabla h(x_c)=\sum_{i=1}^mf_i(x_c)\nabla f_i(x_c)=J(x_c)^TF(x_c)$
\end{problem}
\begin{definition}[Second Order Model for Nonlinear Problem]
  $h(x+\nabla x)\sim h()$ \textcolor{red}{upload later}
\end{definition}
\begin{definition}[Argmin and argmax]
  $\text{argmin } f(x)$ is the set of all minimizers of $f(x)$, similarly $\text{argmax }f(x)$ is the set of all maximizers of $f(x)$.
\end{definition}
\begin{theorem}[Lagrange Multipliers for Equality Constraints]
  Suppose that $f: C \to \mathbb R$ and $h: C \to \mathbb R^m$ are sufficiently smooth functions on the open set $C \subseteq \mathbb R^n$. Let $x \in C$ be a local minimum of $f$ subject to the constraints $h(x) = 0, x \in C$. In addition, assume the following regularity condition at $x$ (constraint qualification at $x$) $$h'(x) \text{ is onto}$$
  Then there exists $\lambda \in \mathbb R^m$ (a Lagrange multiplier vector) such that $$0 = \nabla f(x) + \langle \lambda, h'(x) \rangle$$ Equivalently, the gradient $\nabla f(x)$ is in the range of $h'(x)^T$.
\end{theorem}