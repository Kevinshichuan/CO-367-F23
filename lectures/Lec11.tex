\subsection{Lecture 11+ cts}
\subsubsection{Convergence of Line Search Methods}
\begin{definition}[Key Quantity]
    The key quantity is the angle between the search direction and the gradient. It is
    $$\cos \theta_k = \frac{\langle \nabla f(x_k), v_k \rangle}{\| \nabla f(x_k) \| \| v_k \|}$$
    We will use this together with Wolfe conditions to prove convergence.
\end{definition}
\begin{theorem}[Convergence of Line Search Methods]
    Let $x_0 \in \mathbb R^n \to \mathbb R$ be $C^1$-smooth, and $f$ be bounded below. Let $0 < c_1 < c_2 < 1$ and suppose that there exists a Lipschitz constant $L > 0$ such that $$ \| \nabla f(x) - \nabla f(y) \| \leq L \| x - y \|$$
    on an open set containing the level set $\{z: f(z) \leq f(x_0)\}$.

    Let $\{x_k\}$ be generated using descent directions $v_k$ and step lengths that satisfy the Wolfe conditions. Then we have
    $$\sum^\infty_{k=1} \cos^2 \theta_k \|\nabla f(x_k) \|^2 < \infty$$
\end{theorem}
\begin{proof}[Proof]
    In course notes.
\end{proof}
\begin{corollary}
    If there exists $\delta > 0$ satisfying $\cos^2 \theta_k \geq \delta > 0$ for all $k$, then 
    $$\lim_{k \to \infty} \nabla f(x_k) = 0$$
\end{corollary}

\subsubsection{Convergence Rate of Steepest Descent}
For exact line search, we have
$$x_{k+1} = x_k - \left(\frac{\| \nabla f(x_k)\|^2}{\langle A \nabla f(x_k), \nabla f(x_k) \rangle}\right) \nabla f(x_k)$$
If we plot out our iterations, we can see that there is a zig-zagging behaviour and the directions $x_{k+1} - x_k$ and $x_k - x_{k-1}$ are orthogonal.
\begin{theorem}[Zig Zagging of Steepest Descent]
    For the general steepest descent method with an exact step-size, we have
    $$(x_{k+1} - x_k)^T (x_k - x_{k-1}) = 0$$
    So they are orthogonal.
\end{theorem}
\begin{theorem}[Rate of Convergence of Steepest Descent]
    Suppose that $f: \mathbb R^n \to \mathbb R$ is $C^2$-smooth and the iterates $\{x_k\}$ generated by steepest descent with exact line search converge to $x^*$, that is, $\lim_{k \to \infty} x_k = x^*$. Suppose that $\nabla^2 f(x^*)$ is positive definite. Then for all large $k$, we have
    $$f(x_{k+1}) - f(x^*) \leq K (f(x_k) - f(x^*))$$ with 
    $$0 \leq K = \left(\frac{\lambda_1 - \lambda_n}{\lambda_1 + \lambda_n}\right)^2 < 1$$
    where $\lambda_1$ and $\lambda_n$ are the largest and smallest eigenvalues of $\nabla^2 f(x^*)$. 
\end{theorem}

\subsubsection{Convergence Rate of Newton's Method}

\begin{definition}[Comparing Algorithms]
    \textcolor{red}{upload later}
  \end{definition}
  
  \begin{definition}[p-th order convergence]
      If residuals $r_R\to 0$ and 
      $lim\frac{||r_{k+1}||}{||r_k||^p}\to r >0$ is called p-th order convergence.
      \\ If p=1,r<1,then this is linear convergence, (For s.d, linear , r $\cong r$ (cond(Hessian at $x^*$)))
      \\ If p =2, quadratic convergence
  \end{definition}
  
  \begin{theorem}[Convergence of Newton]
      Suppose $f:R^n\to R\,in\, c^2-Smooth$, $\nabla^2f(x^*)>0,\nabla f(x^2)=0$, and 
      the hessian locally lipschitz continuous.\\
      Consider iterately:
      $$x_{k+1}=x_k+t_kv_k,v_k=-\nabla^2f(x_k)^{-1}\nabla f(x_k)(Newton\,direction)$$
      Where $t_k$ satisfy wolfe condition, then  if the ""\textcolor{red}{text} point $x_0$,
      "close enough" to $x^*$ ...Theorem 5.6.2 \textcolor{red}{text finish it after}
  \end{theorem}
  \begin{problem}
      $d_{SC}=-\nabla f(x_k),d_N=-\nabla^2f(x_k)^{-1}\nabla f(x_k) $
  \end{problem}
  
  %\begin{Problem}
      levenberg marquardt algorithm
  $-(\nabla^2 f(x_k)+\lambda I)^{-1}\nabla f(x_k)$ $\lambda >0$
  %\end{Problem}
  \begin{problem}
      This leads to trust region method ,
      follows by 5.8.1 Trust Region Methods Outline.
      note: Find $d_k$ is trust region subproblem
  \end{problem}



To finish:

Newton/trust region (second order model)
\subsubsection{Trust Region Strategy}
\begin{problem}[Trust Rigion Strategy]
  Construct a model for f at $x_k$, $f(x_k+\Delta x)\sim mq(\Delta x)=f(x_k)+\Delta x^T\nabla f(x_k)+\frac{1}{2}\Delta x^TB_k\Delta x$,
  where $B_k\simeq \nabla^2 f(x_k)$. COnstruct region where we trust the model. Region can be a ball,
  eg. $\Omega_k=\{\Delta x| ||\Delta x||\leq \delta_k^2\}$
\end{problem}
The difference in trust region strategy and line search strategy is that in line search strategy, we first choose the direction, then choose step size. In trust region, we first choose the step size, then we choose the direction.
\begin{definition}[Trust Region Strategy]
  In each iteration, we construct a model of $f$. That is, in each step we consider $m_k: \mathbb R^n \to \mathbb R$ that is a simple function that approximates $f$ well on some simple set $\Omega_k$ (the trust region) around our current approximation $x_k$. Then we find the new approximation $$\hat x = \text{argmin } m_k(x) \quad \text{$x \in \Omega_k$}$$
  A common model is the quadratic model $$m_k(x) = f(x_k) + \langle \nabla f(x_k), x - x_k \rangle + \frac{1}{2} \langle x - x_k, B_k(x - x_k) \rangle$$ where $B_k \approx \nabla^2 f(x_k)$ approximates the Hessian. If the values $f(\hat x)$ and $m_k(\hat x)$ are close, then we declare $x_{k+1} = \hat x$. Otherwise, we shrink the size of the trust region $\Omega_k$ and repeat the process.

  Usually $\Omega_k$ is a ball, ellipsoid, or a box around $x_k$.

  The main points are how to choose a model function $m_k$, and how to choose a trust region $\Omega_k$.
\end{definition}




\section{Convex set and function}
\subsection{Lecture 11+ cts}
\begin{definition}[Line Segment]
    Let $x,y \in \mathbb R^n$. The line segment joining $x$ and $y$ is the set
    $$[x,y] = \{\lambda x + (1 - \lambda)y : \lambda \in [0,1]\}$$
\end{definition}

\begin{definition}[Convex set]
    Set $S$ in $R^n$ in a convex set if for every $x, y \in S$ and for every  $0\leq \lambda \leq 1$ $$\lambda x+(1-\lambda)y\in S$$
    That is, the line segment $[x,y]$ is contained in $S$.
\end{definition}
\begin{definition}[Convex combination]
    Let $S$ be a convex set and let $x_1,\ldots x_n \in S$. If $\lambda_1,\lambda_2,\ldots,\lambda_n$ are nonnegative numbers such that $$\sum_{i=1}^{n} \lambda_i = 1$$ then $$\sum^n_{i=1} \lambda_i x_i$$ is called a convex combination and is contained in $S$.
\end{definition}

\begin{lemma}
    $S$ is a convex set iff $S$ contains all its convex combination
\end{lemma}
\begin{definition}
    Given $S\in R^n$, the convex hull of $S$, $\text{conv }S$ is the smallest convex set containing $S$ and is equal to the set of all convex combinations of points in $S$.
\end{definition}

\begin{lemma}
    Suppose $S$ is a compact set, then $\text{conv }S$ is a compact set.
\end{lemma}
\begin{definition}[Affine Set]
    Let $S \subseteq \mathbb R^n$ be a subspace, and $a \in \mathbb R^n$. Then $L = a + S$ is an affine set (i.e. it is a translation of a subspace).

    Moreover, $\dim L = \dim S$. In addition, let $T \subseteq \mathbb R^n$. Then the affine hull, $\text{aff }T$, is the smallest affine set containing $T$. If $T$ is a convex set, then $\dim(T) = \dim(\text{aff }T)$.
\end{definition}
\begin{definition}[Convex Cone]
    $K \subseteq \mathbb R^n$ is a convex cone if the Minkowski sums satisfy
    \begin{enumerate}
        \item $\lambda K \subseteq K$ for all $\lambda \geq 0$
        \item $K + K \subseteq K$
    \end{enumerate}
\end{definition}