\newpage
\section{Constrained Optimization}
\subsection{Lectures 16-17}
\subsubsection{Review}
Recall that constrained optimization problems can be formulated as 
\begin{align*}
    \min \quad & f(x) \\
    \text{s.t.} \quad &g_i(x) \leq 0, \quad i \in \{1,\ldots,m\} \\
    & h_j(x) = 0, \quad j \in \{1,\ldots,p\}
\end{align*}
$f: \mathbb R^n \to \mathbb R$ is the objective function, $x \in \mathbb R^n$ is the optimization variable. The inequalities $g_i(x) \leq 0$ are called inequality constraints, the equations $h_j(x) = 0$ are called the equality constraints. If there are no constraints, i.e. $m = p = 0$ then we have an unconstrained optimization problem.

\bigskip
A special case of constrained optimization is linear programming:
\begin{align*}
    \min \quad & c^T x \\
    \text{s.t.} \quad & Ax = b \in \mathbb R^n\\
    & x \geq 0
\end{align*}
In this case, the equality constraints are $$h_i(x) = A_i x - b_i$$ where $A_i$ is the $i$th row vector and the inequality constraints are $$g_i(x) = -x_i$$ For an LP, the feasible region is a polyhedron. Recall that a polyhedron is a set of the form
$$P = \{x \in \mathbb R^n: Ax \leq b\}$$ where $A$ is a $m \times n$ matrix and $b \in \mathbb R^m$.
\subsubsection{Linear Programming, Lagrange Multipliers, and Implicit Function Theorem}
\begin{example}[Simplex Method]
    Consider the following LP in standard form:
    \begin{align*}
        \min \quad & c^T x \\
        \text{s.t.} \quad & Ax = b \in \mathbb R^n\\
        & x \geq 0
    \end{align*}
    where $A$ has full row rank (constraints are in linear independence (LICQ)).

    Let $x = \begin{pmatrix}x_B \\ x_N \end{pmatrix}$ denote a basic feasible solution, BFS. So $x$ is feasible and $x_N = 0$. 
    
    We call $x_B$ the basic variables and $x_N$ the non-basic variables. 
    
    We let $A = \begin{pmatrix}B & N\end{pmatrix}$ where $B$, the basis matrix, is an $m \times m$ invertible matrix. W.L.O.G. we have assumed the first $m$ variables to be the basic variables, so the basis is $B = \{1,\ldots,m\}$.

    We split the constraints $Ax = b$ into two parts, $$Bx_B + Nx_N = b$$ 
    We solve our equation w.r.t. $x_B$ to get $$x_B = B^{-1}b - B^{-1}Nx_N$$
    Similarly, we can split our objective function $c^T x$ into two parts, $$c^T_B x_B + c^T_N x_N$$ Substituting $x_B = B^{-1}b - B^{-1}Nx_N$ into objective function gives
    $$c^T_B x_B + c^T_N x_N = c^T_B(B^{-1}b - B^{-1}Nx_N) + c^T_N x_N$$
    Then let $y^T = c^T_BB^{-1}$ denote the dual variable estimates. The objective function becomes
    \begin{align*}
        c^Tx &= y^Tb - y^TNx_N + c^T_Nx_N \\
        &= y^Tb + (c^T_N - y^TN)x_N
    \end{align*}
    We can see that the objective values decreases if we find a reduced cost $c^T_N - y^TN < 0$ and increase the corresponding nonbasic variable $x_j$, i.e. $j$ enters the basis. Review Simplex method if needed.
\end{example}
\begin{example}
    Example of Lagrange multipliers and implicit function thoerem here that I do not understand.
\end{example}
\begin{theorem}[Lagrange Multipliers for Equality Constraints]
    Suppose that $f: C \to \mathbb R$ and $h: C \to \mathbb R^m$ are sufficiently smooth functions on the open set $C \subseteq \mathbb R^n$. Let $x \in C$ be a local minimum of $f$ subject to the constraints $h(x) = 0, x \in C$. In addition, assume the following regularity condition at $x$ (constraint qualification at $x$) $$h'(x) \text{ is onto}$$
    Then there exists $\lambda \in \mathbb R^m$ (a Lagrange multiplier vector) such that $$0 = \nabla f(x) + \langle \lambda, h'(x) \rangle$$ Equivalently, the gradient $\nabla f(x)$ is in the range of $h'(x)^T$.
\end{theorem}
\subsubsection{Optimality Conditions and Geometry}
Consider the following LP:
\begin{align*}
    \min \quad & f(x) \\
    \text{s.t.} \quad & x \in Q
\end{align*}
where $f: \mathbb R^n \to \mathbb R$ is $C^1$ or $C^2$ smooth and $Q$ is a closed set.

We say that $\bar x$ is a local minimizer if there exists $\epsilon > 0$ such that $$\forall x \in Q \cap B_\epsilon(\bar x), \quad f(x) \geq f(\bar x)$$ where $B_\epsilon(\bar x)$ is a ball of radius $\epsilon$ centered at $\bar x$. For an NLP, the feasible set is 
$$Q = \{x: g_i(x) \leq 0, h_j(x) = 0, i \in I, j \in E\}$$
where $I,E$ are index sets for the inequality and equality constraints respectively.

For a convex program, the feasible set is a convex set.
\begin{example}
    If we have the LP
    \begin{align*}
        \min \quad & f(x) \\
        \text{s.t.} \quad & c_1(x_1,x_2) = x_1^2 + x_2^2 = 0 
    \end{align*}
    Let $x^*$ be a local optimal solution. Then $\nabla f(x^*) \in \mathrm{span}(\nabla c_i)$ by Lagrange Multiplier since
    $$\nabla f(x^*) + \lambda \nabla c_1(x^*) = 0$$ and rearranging gives
    $$\nabla f(x^*) = -\lambda \nabla c_1(x^*)$$
\end{example} 
\begin{example}
    Consider the LP
    \begin{align*}
        \min \quad & f(x) = x_1 + x_2 \\
        \text{s.t.} \quad & c_1(x_1,x_2) = 1 - x_1^2 - x_2^2 \geq 0 
    \end{align*}
    Let $\bar x$ be a local optimal solution, then
    \begin{enumerate}
        \item If $\bar x$ satisfies $c_1(\bar x) > 1$, then $\nabla f(\bar x) = 0$.
        \item If $\bar x$ satisfies $c_1(\bar x) = 1$, then $\nabla f(\bar x) \in \underbrace{\mathrm{cone}\{\nabla c_1(\bar x)\}}_{-\lambda \nabla c_1(\bar x), \lambda \leq 0}$.
    \end{enumerate}
\end{example}
\begin{definition}[Active Set]
    Let $\bar x$ be a feasible solution of an NLP. Then the set
    $$A(\bar x) = \{i \in I: g_i(\bar x) = 0\}$$ is called the active set at the feasible point $\bar x$. The inequality constraint $i \in I$ is said to be active at a feasible point $\bar x$ if $g_i(\bar x) = 0$ and inactive if $g_i(\bar x) < 0$.
\end{definition}
\begin{theorem}[]
    Let $\bar x$ be a local minimizer of 
    \begin{align*}
        \min \quad & f(x) \\
        \text{s.t.} \quad & g_i(x) \leq 0, \quad i \in I \\
    \end{align*}
    where $f, g_1,\ldots,g_m \in C^1$. Then there does not exist vector $d$ such that 
    $$\nabla f(\bar x)^Td < 0 \quad \text{and} \quad \nabla g_i(\bar x)^Td < 0, \quad \forall i \in A(\bar x)$$
    where $A(\bar x) = \{i \in I: g_i(\bar x) = 0\}$ is the active set at $\bar x$.
\end{theorem}
\begin{proof}[Proof]
    Suppose such $d$ exists. Then by Taylor's Theorem, there exists $\alpha' > 0$ such that for every $\alpha \in (0, \alpha')$
    \begin{align*}
        f(\bar x + \alpha d) &< f(\bar x) \tag*{and} \\
        g_i(\bar x + \alpha d) &< g_i(\bar x) = 0 \quad \forall i \in A(\bar x) 
    \end{align*}
    which contradicts the fact that $\bar x$ is a local minimizer.
\end{proof}
\begin{lemma}
    Let $A \in \mathbb R^{m \times n}$. Then exactly one of the following statements holds:
    \begin{enumerate}
        \item There exists $x \in \mathbb R^n$ such that $Ax < 0$
        \item There exists $y \in \mathbb R^m$ such that $A^Ty = 0$ and $y \geq 0$
    \end{enumerate}
\end{lemma}
\begin{theorem}[]
    Let $\bar x$ be a local minimizer of 
    \begin{align*}
        \min \quad & f(x) \\
        \text{s.t.} \quad & g_i(x) \leq 0, \quad i \in I \\
    \end{align*}
    where $f, g_1,\ldots,g_m \in C^1$. Assume that the set $\{\nabla g_i(\bar x): i \in A(\bar x)\}$ is linearly independent. Then there exists $\lambda_1,\ldots,\lambda_m \geq 0$ such that
    \begin{align*}
        \nabla f(\bar x) + \sum^m_{i=1} \lambda_i \nabla g_i(\bar x) &= 0 \\
        \lambda_i g_i(\bar x) &= 0 \quad \forall i \in I
    \end{align*}
\end{theorem}
\begin{definition}[LICQ]
    Given $\bar x$ and $A(\bar x)$, we say that the linear independence constraint qualification (LICQ) holds if the set of active constraint gradients
    $$\{\nabla g_i(\bar x): i \in A(\bar x)\} \cup \{\nabla h_j(\bar x): j \in E\}$$ is linearly independent.
\end{definition}

\begin{definition}[Cone]
    We say that the set $K$ is a cone if for every $x \in K$ and for every $\alpha > 0$, we have $\alpha x \in K$.
\end{definition}
\begin{definition}[Tangent Vector]
    Let $\bar x \in Q \subseteq \mathbb R^n$. A vector $v \in \mathbb R^n$ is tangent to $Q$ at $\bar x \in Q$ if there exists sequences $\{x_k\}\subseteq Q$ and $\{t_k\} \subset R_+$ such that $$x_k \to \bar x, \quad \underbrace{t_k \downarrow 0}_{t_k > 0, t_k \to 0, \text{non-increasing}}, \quad \frac{1}{t_k}(x_k - \bar x) \to v$$
\end{definition}
\begin{definition}[Tangent Cone]
    We denote the set of all tangent vectors to $Q$ at $\bar x$ as the tangent cone $T_Q(\bar x)$.
\end{definition}
\begin{theorem}[Geometric First-Order Necessary Condition, Rockafellar-Pshenichnyi]
    Let $f$ be sufficiently smooth on the set $\Omega \subseteq \mathbb R^n$, and let $x^*$ be a local minimizer of 
    \begin{align*}
        \min \quad & f(x) \\
        \text{s.t.} \quad & x \in \Omega
    \end{align*}
    Then $$\langle \nabla f(x^*), v \rangle \geq 0, \quad \forall v \in T_\omega(x^*)$$
    where $T_\omega(x^*)$ is the tangent cone to $\omega$ at $x^*$.
\end{theorem}
\begin{definition}[Polar Cone]
    Given a cone $A$, the polar cone $A^+$ is defined as a set of vectors that make a non-obtuse angle with every vector in $A$. Formally, if $A$ is a cone in a real vector space, the polar cone $A^+$ is defined as
    $$A^+ \{y \in \mathbb R^n: \langle y, x \rangle \leq 0 \quad \forall x \in A\}$$
\end{definition}
\begin{definition}[Normal Cone]
    The normal cone of the set $\Omega$ at $\bar x \in \Omega$ is $$N_{\Omega}(\bar x) = -(T_\Omega(\bar x))^+$$ 
    where $T_\Omega(\bar x)^+$ is the polar cone of $T_\Omega(\bar x)$.
\end{definition}
\begin{corollary}
    Let $f$ be sufficiently smooth on the set $\Omega \subseteq \mathbb R^n$, and let $x^*$ be a local minimizer of 
    \begin{align*}
        \min \quad & f(x) \\
        \text{s.t.} \quad & x \in \Omega
    \end{align*}
    Then $$\nabla f(x^*) \in N_\Omega(x^*)$$
\end{corollary}
\begin{definition}[Linearized Feasible Directions/Linearizing Cone]
    Given an NLP with a feasible set $F$, the set of linearized feasible directions(linearizing cone) at $\bar x \in F$ is defined as 
    $$L_F(\bar x) = \{d: d^T \nabla g_i(\bar x) \leq 0, \forall i \in A(\bar x) \cap I, \quad d^T \nabla h_j(\bar x) = 0, \forall j \in E\}$$
\end{definition}
\begin{definition}[Lagrangian Function for NLP]
    We define the Lagrangian function for an NLP as 
    \begin{align*}
        \mathcal L(x, \lambda, \mu) &= f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{j=1}^p \mu_j h_j(x) \\
        &= f(x) + \lambda^T g_I(x) + \mu^T h_E(x)
    \end{align*}
    where $g_I(x)$ are the inequality constraints and $h_E(x)$ are the equality constraints.
\end{definition}
\begin{theorem}[First Order KTT Necessary Condition]
    Suppose $x^*$ is a local minimizer of an NLP and $f,g_1,\ldots,g_m,h_1,\ldots,h_p \in C^1$ and LICQ holds at $x^*$. Then there exists Lagrangian multipliers $\lambda^* \in \mathbb R^m, \mu^* \in \mathbb R^p$ such that
    \begin{align*}
        \nabla_x \mathcal L(x^*, \lambda^*, \mu^*) &= 0, \quad \lambda^* \geq 0 \tag*{Dual feasibility} \\
        g_i(x^*) &\geq 0, \quad i \in I, \quad h_j(x^*) = 0, \quad j \in E \tag*{Primal feasibility} \\
        \lambda^*_i g_i(x^*) &= 0, \quad i \in I \tag*{Complementary slackness}
    \end{align*}
    We call the dual feasibility, primal feasibility and complementary slackness conditions as KTT conditions, and call the point satisfying these conditions as KTT point.
\end{theorem}
