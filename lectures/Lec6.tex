\subsection{Lecture 6}
Goal: Solving normal equation/non linear case
\begin{definition}[Singular Values of a Matrix]
  The singular values of a matrix $A$ are the square roots of the eigenvalues of the matrix $A^TA$. They are always non-negative real numbers.

  The number of non-zero singular values of a matrix equals the rank of that matrix.
\end{definition}
\begin{definition}[Condition Number of a Matrix]
  Suppose $A \in \mathbb R^{m \times n}, m > n$ is full column rank. The condition number of the matrix $A$, $\text{cond}(A)$, is the ratio of the largest to smallest nonzero singular values of $A$. Let $\sigma_{\max}$ be the largest singular value and $\sigma_{\min}$ be the smallest singular value. Then
  $$\text{cond}(A) = \frac{\sigma_{\max}}{\sigma_{\min}}$$
\end{definition}
\begin{definition}[\href{https://www.geeksforgeeks.org/singular-value-decomposition-svd/}{SVD Decomposition}]
  Let $A$ be an $m \times n$ matrix. Then $A$ can be factored into
  $$\underbrace{A}_{m \times n} = \underbrace{U}_{m \times m}\underbrace{\Sigma}_{m \times n}\underbrace{V^T}_{n \times n}$$
  where
  \begin{itemize}
    \item $U$ is an $m \times m$ orthogonal matrix consisting of eigenvectors of $AA^T$
    \item $V^T$ is the transpose of an $n \times n$ matrix containing the eigenvectors of $A^TA$
    \item $\Sigma$ is a diagonal matrix with $r = \text{rank}(A)$ positive eigenvalues of $AA^T$ (Singular values of $A$) on the diagonal.
  \end{itemize}
\end{definition}
\textcolor{red}{there is a section on piazza posted lecture notes that shows why using SVD decomposition to solve normal equation is a bad idea. Not sure if i should include here}
\begin{definition}[Orthogonal Matrix]
  A matrix $Q$ is orthogonal if $Q^TQ = I$.
\end{definition}
\begin{definition}[Orthonormal Columns]
  A matrix $Q$ has orthonormal columns if each column vector is a unit vector (norm is 1), and any two distinct columns are orthogonal (inner product is 0).
\end{definition}


\begin{problem}
  \section*{Applicable Model for "best" data fitting}
  \[ (s_i, p_i), \quad i = 1,...,m \]
  \begin{align*}
  \text{We got linear model and we need to solve } & A^T A x = A^T b \text{ normal equation} \\
  \text{Today: Solve normal equation (whenever case)} \\
  \text{i.e., this is from optimality condition for } \\ 
  & \text{min } \frac{1}{2}\|Ax-b\|_2^2 = f(x) \\ \text{, s.t.} 
  A & \in M_{m,n} \\
  x & \in \mathbb{R}^n \\
  \text{stationary: } \nabla f(\bar{x}) & = A^T(A\bar{x}-b) = 0 \\
  \text{An alternate view is min } \|y-b\| & \\ \text{, s.t. } y \in \text{Range}(A) 
  & \Rightarrow \text{ a projection problem} \\
  & \text{i.e., project } b \text{ onto Range of } A \\
  \text{Aside: use SVD of } A=U\Sigma V^T & : \Sigma = \begin{bmatrix}
  \sigma_1 & & 0 \\
  & \ddots & \\
  0 & & \sigma_r \\
  & 0 &
  \end{bmatrix}, \quad r = \text{rank}(A)\, \sigma_i >0
  \end{align*}
  
  
  
  
  
  SVD can be used for a "rank revealing" decomposition.
  
  \[
  U = \begin{bmatrix} U_1 & U_2 \end{bmatrix}
  \]
  
  Where:
  \begin{align*}
  U_1 & : \text{m x r orthogonal columns span R(A)} \\
  U_2 & : \text{m x (m-r) spanning } \mathcal{N}(A^T)
  \end{align*}
  
  Properties:
  \[
  U^TU = I \quad \text{and} \quad I= U_1^TU_1 =( U_1^TU_1)^T = \mathcal{C}(U_1U_1^T) = U_1U_1^T
  \]
  
  This means $U_1U_1^T$ is symmetric idempotent, i.e., orthogonal projection.
  
  To solve the projection minimizing $\| y - b \|$ where $y \in \text{Range}(\mathcal{R}(A))$:
  \[
  \text{optimal } y^* = U_1U_1^Tb
  \]
  
  Defining the Moore-Penrose generalized inverse:
  \[
  A^+ = V \Sigma^+ U^T
  \]
  where
  \[
  \Sigma^+ = \begin{bmatrix} \frac{1}{\sigma_1} & \dots & 0 \\ 0 & \dots & 0 \end{bmatrix}
  \]
  and
  \[
  \gamma_i = \frac{b_i}{\sigma_i} \quad \text{if } \sigma_i \neq 0
  \]
  
  Finally, to find $x$ such that $Ax = y^* = U_1U_1^Tb = AA^+b$:
  If $A$ has full column rank (i.e., $\mathcal{R}(A^T) = \mathbb{R}^n$), then $x$ is unique.
  Otherwise $\bar{x}+v,v\in N(A)$ is a solution for all v in N(A)
  Choosing a large solution. This solution of min is called "Best least square solution"
  \\ we can write it as bilevel problem
  
  
  
  
  The solution of minimum norm is called the \textit{"best least square solution"}. This can be represented as a "bilevel problem":
  \[
  \min_{\hat{x}} \|\hat{x}\|_2 \quad \text{for values when different}
  \]
  subject to:
  \[
  x \in \arg\min_{z} \frac{1}{2} \|Ax - b\|_2^2
  \]
  
  The bLSS (best least square solution) is the unique LSS (least square solution) in $\mathcal{R}(A^T)$.
  \end{problem}
  \begin{definition}[Condition number]
    
  
  Given:
  \[
  A = U \Sigma V^T \quad (\text{SVD of } A)
  \]
  Where $\Sigma$ is a diagonal matrix with:
  \[
  \Sigma = \begin{pmatrix} \sigma_1 & 0 \\ 0 & \sigma_r \end{pmatrix} \quad \text{and} \quad \sigma_r \geq 0
  \]
  
  The condition number of matrix $A$ is given by:
  \[
  \text{Cond}(A) = \frac{\sigma_1}{\sigma_r}
  \]
  
  If $A$ is not of full rank, then:
  \[
  \text{Cond}(A) = \infty \implies   \text{Cond}(A^T) = \text{Cond}(A)
  \]
  
  We also have:
  \[
  A^TA = V \Sigma^T U^T U \Sigma V^T = V \Sigma^2 V^T
  \]
  With:
  \[
  \Sigma^2 = \begin{pmatrix} \sigma_1^2 & 0 \\ 0 & \sigma_r^2 \end{pmatrix}
  \]
  Thus:
  \[
  \text{Cond}(A^TA) = \frac{\sigma_1^2}{\sigma_r^2}=Cond(A)^2
  \]
  
  For the solution $B$:
  \[
  Bx^*(True\, sol) =b, B\hat{x}\,(Numerical) = b + \delta b
  \]
  Where:
  \[
  \frac{\|x^* - \hat{x}\|}{\|x^*\|} \leq \text{Cond}(B) \frac{\delta \|b\|}{\| b\|}
  \]
  
  
  \end{definition}
  
  To avoid solving the normal equation with sequares condition number, we should use QR
  
  
  

  
\begin{definition}[QR Factorization]
  For any $m \times n$ matrix $A$, there there exists an $m \times m$ orthogonal matrix $Q$ ($QQ^T = I$) and an $m \times n$ upper triangular matrix $R$ ($R_{i,j} = 0, \forall i < j)$ satisfying $A = QR$. Moreover, if the columns of $A$ are linearly independent then we can get
  \begin{align*}
    A &= QR \\
    &= Q \begin{bmatrix}
      R_1 \\ 0
    \end{bmatrix} \\
    &= \begin{bmatrix}
      Q_1 & Q_2
    \end{bmatrix} \begin{bmatrix}
      R_1 \\ 0
    \end{bmatrix} \\
    &= Q_1R_1
  \end{align*}
  where 
  \begin{itemize}
    \item $R_1$ is an invertible $n \times n$ upper triangular matrix
    \item $0$ is an $(m - n) \times n$ zero matrix
    \item $Q_1$ is an $m \times n$ matrix with orthonormal columns
    \item $Q_2$ is an $m \times (m - n)$ matrix with orthonormal columns
  \end{itemize}
\end{definition}
\begin{theorem}[QR Factorization on Normal Equation]
  Assuming that the columns of $A$ are linearly independent, then the normal equation $A^TAx = A^Tb$ can be solved by applying QR factorization to $A$:
  \begin{align*}
    (A^TA)x &= A^Tb \\
    ((Q_1R_1)^TQ_1R_1)x &= (Q_1R_1)^Tb \\
    (R_1^TQ_1^TQ_1R_1)x &= R_1^TQ_1^Tb \\
    R_1^TR_1x &= R_1^TQ_1^Tb \tag*{Since $Q_1$ is orthogonal}\\
    R_1x &= Q_1^Tb \tag*{Since $R_1$ is invertible}
  \end{align*}
\end{theorem}
\begin{definition}[Methods of Solving General Linear Systems]
  Suppose we are given a linear system $Bx = b$, and we know that this system has a solution, i.e. $b \in \text{range}(B)$. There are 3 important algorithms/factorizations used to find $x$:
  \begin{itemize}
    \item Gaussian Elimination (LU factorization) ($PB = LU$)
    \item QR factorization
    \item SVD, singular value decomposition
  \end{itemize}
\end{definition}
\begin{problem}[Solving Large Positive Definite Systems]
  Suppose we have a linear system, $Ax = b$, with $A$ postive definite. If $x^*$ is a solution, then $Ax^* - b = 0$. Then this is equivalent to minimizing the function $$f(x) = \frac{1}{2} \|A_x - b\|^2, \nabla f(x) = Ax - b = 0$$ \textcolor{red}{Dont understand this and the part after as well. You will have to add more notes here. Link to notes \href{https://cdn-uploads.piazza.com/paste/kcat3sa7dyu2te/bf8109b64a3bff7a1ffbacd0a5d43d03fabf0c01350afdf12f55b5382b6571da/Lecture_6.pdf}{HERE}}
\end{problem}
\begin{definition}[Steepest Descent Direction in First order model]
  The steepest descent direction in the first order model is d=-$\frac{\nabla f(x_c)}{|x_c|}$
  
\end{definition}
\begin{theorem}[Conjugate Gradient Method]
  The first search direction is the negative gradient, $$v_0 = -\nabla q(x_0)$$ with $q = f$. At the $k$th iteration:
  $$v_{k+1} = -\nabla q(x_k) + \beta_k v_k$$
  where $\beta_k$ is chosen to ensure $\langle Av_{k+1}, v_k \rangle = 0$. This guarantees that the directions are $A$-conjugate \textcolor{red}{wtf is $A$ conjugate $u,v\,conjugate,if\,u^TAv=0$}. We then set $$x_{k+1} = x_k + \alpha_{k+1}v_{k+1}$$ where $\alpha_{k+1}$ is chosen from an exact line search (\textcolor{red}{what is line search}).
\end{theorem}


\begin{problem}
  \begin{align*}
    \text{min} \quad f(x)  \\
    x & \in D \subset \mathbb{R}
    \end{align*}
\end{problem}


\begin{itemize}
\item 1\textsuperscript{st} \& 2\textsuperscript{nd} order optimality condition both necessary and sufficient
\item 1\textsuperscript{st}, 2\textsuperscript{nd} order model
\end{itemize}