\begin{problem}[DNE of multiplier]
  \textbf{Recall:}
\begin{align*}
\text{minimize } & f(x) \\
\text{subject to } & f_i(x) = 0, \quad i = 1, \dots, m \\
L(x, \lambda) &= f(x) - \sum_{i=1}^m \lambda_i f_i(x) \quad \text{(Lagrangian multiplier)}
\end{align*}

\text{If } $x^* \text{ is a local minimum and }$
\begin{align*}
\left\{ \nabla f_i(x^*) \right\} \text{ is a linearly independent set, then }
\nabla L(x^*, \lambda) &= \nabla f(x^*) + \sum_{i=1}^m \lambda_i \nabla h_i(x^*) = 0 \text{ for some } \lambda \in \mathbb{R}
\end{align*}

\textbf{Example:}
\begin{align*}
f(x) &= x \\
h(x) &= x^2 = 0 \\
L(x, \lambda) &= x + \lambda x^2, \quad \text{with constraint } x^2 = 0 \\
0 &= \nabla L(x, \lambda) = 1 + 2 \lambda x=1+0. \text{ No $\lambda$ exist}
\end{align*}

\end{problem}
\section{Iterative Methods for Unconstrained Optimization}
\subsection{Lecture 8-10}
Before, we have iterative methods to solve linear and linear least squares system.

\textbf{Goal}: Solve more general problems of minimization.

An iterative algorithm is a procedure that produces an (infinite) sequence of points $\{x_k\}$, in $\mathbb R^n$ such that our sequence converges to a critical point of $f$ or to a point that satisfies the second order necessary conditions of optimality.

\subsubsection{Line Search Strategy}
Most Line Search Algorithms have the following procedure:

Choose a starting point $x_0$. For $k = 0,1,2,\ldots$
\begin{enumerate}
  \item Choose a search direction $d_k$.
  \item Choose a step length $\alpha_k > 0$ (that satisfies $f(x_k + \alpha_k d_k) < f(x_k)$)
  \item Update $x_{k+1} = x_k + \alpha_k d_k$
  \item Stop if a stopping criterion is satisfied.
\end{enumerate}
\begin{definition}[Line Search Strategy]
  At each iteration $k$, we choose a vector (search direction) $d_k \neq 0 \in \mathbb R^n$, then choose $\alpha_k > 0$ (step length) that approximately solves
$$\alpha_k \in \text{argmin } f(x_k + \alpha d_k)$$
Then we update $x_{k+1} = x_k + \alpha_k v_k$.
\end{definition}
\subsubsubsection{Finding Descent Direction}
\begin{definition}[Descent Direction]
  Let $x \in U \subseteq \mathbb R^n$ with $U$ being an open set. Then $d \in \mathbb R^n$ is a descent direction for $f$ at $x$ if there exists $\bar \alpha > 0$ such that $$x + \alpha d \in U, f(x + \alpha d) < f(x)$$ for all $0 < \alpha < \bar \alpha$.

  \bigskip So $f(x_{k+1}) < f(x_k)$.
\end{definition}
\begin{lemma}
  Let $f$ be sufficiently smooth on an open set $U$ and let $\bar x \in U$. Let $d \in \mathbb R^n$ satisfy $$\langle d, \nabla f(\bar x) \rangle = \nabla f(\bar x)^T d < 0$$ Then $d$ is a descent direction for $f$ at $\bar x$.

  \bigskip (i.e., the directional derivative of $f$ at $\bar x$ in the direction of $d$ is negative, so we know that it is the descent direction.)
\end{lemma}

\begin{proof}
  From the hypothesis we have
\[
f(x + td) = f(x) + t \nabla f(x) \cdot d + o(t).
\]
Therefore we deduce
\[
\frac{f(x + td) - f(x)}{t} = \langle d, \nabla f(x) \rangle + \frac{o(t)}{t} < 0, \text{ for all sufficiently small } t > 0.
\]
Moreover, since $U$ is open, we have $x + td \in U$ for all sufficiently small $t > 0$.
\end{proof}

\begin{problem}[Example(direction of steepest descent)]
  \textbf{We saw}
  \[
  \min \nabla f(x)^t \cdot d \quad \text{s.t.} \quad ||d|| = 1
  \]
  \textbf{get} 
  \[
  d = -\frac{\nabla f(x)^T}{||\nabla f(x)||}
  \]
  \textbf{Cauchy direction of steepest descent}
  
  \textbf{Alternatively},
  \[
  -\nabla f(x)^T \cdot d \leq ||d|| \cdot ||\nabla f(x)^T|| = ||d|| \cdot f(x)^T
  \]
  \textbf{but} 
  \[
  d = -\nabla f(x)^T \text{ gives equality.}
  \]
  \textbf{So C.S yields s.t direction}
  
  $\textbf{If } B \succ 0, \textbf{then} $
  \[
  d = -B^{-1} \nabla f(x)^T \text{ in a "deflated gradient" direction.}
  \]
  
  
\end{problem}
\begin{proposition}[Existence of B]
  Suppose$\nabla f(\bar{x})^T d <0$ Then there exists$B\succ0$that satisfies d=$-B^{-1}\nabla f(\bar{x})$
\end{proposition}
\begin{proof}
  Ignore it for now\textcolor{red}{text}
\end{proof}

\subsubsection{Second order model: Newton Type}
Model $mq(x)=f(x_c+\nabla f(x_c)^T\Delta x+\frac{1}{2}\Delta x^TB_c \Delta x$ 
where $B_c\simeq \nabla^2f(x_c)$

\begin{lemma}[TFAE]
  1. $mq(\Delta x)$ is bdd below
  \\2. $B_c \succeq 0$ and $\nabla f(x_c)\in range(B_c)$
  \\3. $\Delta \bar{x}=-B_c^+\nabla f(x_c)$ is a global minimum of mq
\end{lemma}
\begin{proof}
  2 to 1:
  \\ Suppose 2 holds, we have $B_c \succeq 0$ and $\exists \bar{\Delta x},B_c\bar{\Delta x}=-\nabla f(x_c))$.
  This is also the stationary condition. Again, since $B_c$ is psd, there exists a psd square root such that $ss=B_c$.
  \\ Then lets complete the square $$mq(\Delta x)=$$ \textcolor{red}{confirm}

\end{proof}
The main difference between line search methods is the choice of the descent direction
\begin{problem}[Example]
  If $\nabla f(x) \neq 0$ then $d = - \nabla f(x)$ is a descent direction since
  $$d^T \nabla f(x) = -\nabla f(x)^T \nabla f(x) = - \|\nabla f(x) \|^2_2 < 0$$
  Another option is letting $d = -H^{-1} \nabla f(x)$ where $H \succ 0$ and $\nabla f(x) \neq 0$ since
  $$d^T \nabla f(x) = - \nabla f(x)^T H^{-1} \nabla f(x) < 0$$
  since $H \succ 0$, we have $H^{-1} \succ 0$, so $\nabla f(x)^T H^{-1} \nabla f(x) > 0$.
\end{problem}

\begin{theorem}[Existence of a Descent Direction]
  Let $f: \mathbb R^n \to \mathbb R$ be continuously differentiable and let $\bar x \in \mathbb R^n$. Assume that $d$ is a descent direction of $f$ at $\bar x$, so $$\nabla f(\bar x)^T d < 0$$ Then there exists $\delta > 0$ such that for every $\alpha \in (0, \delta]$, 
  $$f(\bar x + \alpha d) < f(\bar x)$$.
\end{theorem}
\begin{proof}[Proof]
  By Taylor's theorem, for all $x \in \mathbb R^n$, 
  $$f(x) = f(\bar x) + \nabla f(\bar x)^T (x - \bar x) + o(\|x - \bar x\|)$$
  Then
  $$f(\bar x + \alpha d) = f(\bar x) + \nabla f(\bar x)^T (\bar x + \alpha d - \bar x) + o(\|\bar x + \alpha d - \bar x\|)$$
  Simplifying gives
  $$f(\bar x + \alpha d) = f(\bar x) + \alpha \underbrace{\nabla f(\bar x)^T d}_{< 0} + o(\|\alpha d\|)$$ Rearranging and dividing both sides by $\alpha \|d\|$, we get
  $$\frac{f(\bar x + \alpha d) - f(\bar x)}{\alpha \|d\|} = \frac{\nabla f(\bar x)^T d}{\|d\|} + \frac{o(\|\alpha d\|)}{\alpha \|d\|}$$
  Taking the limit as $\alpha \to 0$, we get
  $$\lim_{\alpha \to 0} \frac{f(\bar x + \alpha d) - f(\bar x)}{\alpha \|d\|} = \frac{\nabla f(\bar x)^T d}{\|d\|}$$
  Since $\nabla f(\bar x)^T d < 0$, we know that the limit is negative. Then there exists $\delta > 0$ such that for all $\alpha \in (0, \delta]$, we have
  $$\frac{f(\bar x + \alpha d) - f(\bar x)}{\alpha \|d\|} < 0$$
\end{proof}

\subsubsubsection{Finding Step Size}
The process of finding the step size $\alpha_k$ is called line search. Some common choices for step size are as follows:
\begin{enumerate}
  \item $\alpha_k = \alpha$ (a constant) for every $k$
  \item Exact line search: $\alpha_k = \text{argmin}_{\alpha \geq 0} f(x_k + \alpha d_k)$
  \item Inexact line search: A step size $\alpha_k$ that achieves a sufficient decrease in $f$. One popular inexact line search conditions are called the Wolfe conditions.
\end{enumerate}
\begin{problem}[Exact Line Search]
  For exact line search, we define a new function
  $$\phi(\alpha) = f(x_k + \alpha d_k)$$
  We can differentiate w.r.t. $\alpha$ using the chain rule to obtain
  $$\phi' (\alpha) = \nabla f(x_k + \alpha d_k)^T d_k$$
  Then we set $\phi'(\alpha) = 0$ to obtain the critical point of $\phi$. Then we can use the second derivative test to determine if the critical point is a local minimizer. If it is, then we have found the optimal step size $\alpha_k$.
\end{problem}
\begin{definition}[Wolfe Conditions]
  Let $0 < c_1 < c_2 < 1$. The Wolfe conditions are
  \begin{enumerate}
    \item Sufficient decrease condition: $$f(x_k + \alpha d_k) \leq f(x_k) + c_1 \alpha \nabla f(x_k)^T d_k$$ or equivalently
    $$\phi(\alpha_k) \leq \phi(0) + c_1 \alpha \phi'(0)$$
    \item Curvature condition: $$\nabla f(x_k + \alpha d_k)^T d_k \geq c_2 \nabla f(x_k)^T d_k$$ or equivalently
    $$\phi'(\alpha_k) \geq c_2 \phi'(0)$$
  \end{enumerate}
\end{definition}
\begin{theorem}[Existence of Step Lengths that Satisfy Wolfe Conditions]
  Assume $f: \mathbb R^n \to \mathbb R$ is continuously differentiable. Let $d_k$ be a descent direction at $x_k$ and assume $f$ is bounded below along the ray $\{x_k + \alpha d_k: \alpha \geq 0\}$. Then if $0 < c_1 < c_2 < 1$ there exist intervals of step lengths satisfying the Wolfe conditions.
\end{theorem}
\begin{proof}[Proof]
  Omitted
\end{proof}
\subsubsubsection{Steepest Descent Method}
The steepest descent method is easy to implement as it requires the calculation of the gradient but not second derivatives. The descent direction at each iteration is chosen as $$d_k = - \nabla f(x_k)$$
\begin{lemma}
  Let $f: \mathbb R^n \to \mathbb R$ be continuously differentiable and let $\bar x \in \mathbb R^n$ such that $\nabla f(\bar x) \neq 0$. Then the optimal solution of the following minimization problem $$\min_{d \in \mathbb R^n} \{\nabla f(\bar x)^T d: \|d\|_2 = 1\}$$ is $$d^* = \frac{-\nabla f(\bar x)}{\|\nabla f(\bar x)\|_2}$$
\end{lemma}
\begin{proof}[Proof]
  By Cauchy-Schwarz inequality, we have
  $$\nabla f(\bar x)^T d \geq -\| \nabla f(\bar x)\|_2 \| d\|_2 = -\|\nabla f(\bar x)\|_2$$
  The smallest value of $d$ is $d = \frac{-\nabla f(\bar x)}{\|\nabla f(\bar x)\|_2}$ since if we plug in $d$ into the inequality, we get
  $$\nabla f(\bar x)^T \frac{-\nabla f(\bar x)}{\|\nabla f(\bar x)\|_2} = \frac{-\|\nabla f(\bar x)\|^2_2}{\|\nabla f(\bar x)\|_2}=-\|\nabla f(\bar x)\|_2$$
  So $d^* = \frac{-\nabla f(\bar x)}{\|\nabla f(\bar x)\|_2}$ is a minimizer of this problem.
\end{proof}
\begin{theorem}[]
  Let $\{x_k\}$ be the sequence generated by the method of steepest descent method where the step sizes are chosen by the exact line search. Then for every $k \geq 0, k \in \mathbb N$ $$(x_{k+2} - x_{k+1})^T (x_{k+1} - x_k) = 0$$
\end{theorem}
\subsubsubsection{Backtracking Line Search}
\begin{definition}[Backtrack Inexact Line Search]
  We make use of the Wolfe Conditions.
  \begin{enumerate}
  \item Initialize $\alpha > 0$, for example, choose $\alpha = 1, c \in (0,1)$
  \item if $(\phi(\alpha) > \phi(0) + c\alpha \phi'(0))$
  \begin{enumerate}
    \item while ($\phi(\alpha) > \phi(0) + c\alpha \phi'(0)$)
    \begin{enumerate}
      \item $\alpha = \alpha / 2$
    \end{enumerate}
  \end{enumerate}
  \item else if ($\phi(2\alpha) \leq \phi(0) + 2c\alpha \phi'(0)$)))
  \begin{enumerate}
    \item while ($\phi(2\alpha) \leq \phi(0) + 2c\alpha \phi'(0)$)
    \begin{enumerate}
      \item $\alpha = 2\alpha$
    \end{enumerate}
  \end{enumerate}
  \item Output $= \alpha$.
\end{enumerate}
\end{definition}
\subsubsubsection{Newton's Method}
In Newton's Method, the search direction is given by $$d_k = -[\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$$
The Newton step is defined only when $\nabla^2 f(x_k)$ is positive definite.
\begin{definition}[Newton's Method]
  Input: Initial point: $x_0$, Tolerance: $\epsilon > 0$
  \begin{enumerate}
    \item Solve the linear system of equations $\nabla^2 f(x_k) d_k = -\nabla f(x_k)$ for $d_k$, so $d_k = -[\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$
    \item Update $x_{k+1} = x_k + \alpha_kd_k$ where $\alpha_k = 1$ for Newton's method
    \item If $\|\nabla f(x_{k+1})\|_2 \leq \epsilon$, stop. Otherwise, go to step 1.
  \end{enumerate}
\end{definition}
\begin{definition}[Operator Norm]
  The operator norm of a matrix $Q \in \mathbb R^{m \times n}$ is defined by 
  $$\|Q\|_2 = \max\{\|Qx\_2: x \in \mathbb R^n, \|x\|_2 = 1\}$$
  This definition satisfies the properties of the norm. For every $A \in \mathbb R^{m \times n}$,
  \begin{itemize}
    \item $\|A\|_2 = 0 \iff A = 0$
    \item $\|\alpha A\|_2 = |\alpha| \|A\|_2$ for every $\alpha \in \mathbb R$
    \item $\|A + B\|_2 \leq \|A\|_2 + \|B\|_2$
  \end{itemize}
\end{definition}
\begin{theorem}[]
  For every $A \in \mathbb R^{m \times n}$, $x \in \mathbb R^n$, we have
  $$\|Ax\|_2 \leq \|A\|_2 \|x\|_2$$
\end{theorem}
\subsubsubsection{Quasi-Newton Methods}
Since computing the Hessian matrix is expensive, we can use an approximation of the Hessian matrix instead. We now set the search direction as
$$d_k = -B_k^{-1} \nabla f(x_k)$$
where the symmetric and positive definite matrix $B_k$ is updated at every iteration by a quasi-Newton updating formula.

\bigskip
There are many quasi-Newton methods, but we will only discuss the BFGS method.

Consider the quadratic model of the objective function:
$$m_k(d) = f(x_k) + \nabla f(x_k)^T d + \frac{1}{2} d^T B_k d$$
where $B_k$ is a positive definite matrix and will be updated at every iteration. The minimizer of this quadratic model is $$d_k = -B_k^{-1} \nabla f(x_k)$$
It is used as the search direction and the new iterate is given by $$x_{k+1} = x_k + \alpha_k d_k$$
where $\alpha_k$ is the step length, chosen so that it satisfies the Wolfe conditions. In this method, we match the gradient of $m_{k+1}$ to the gradient of $f$ at $x_k$ and $x_{k+1}$. That is, we require
$$\nabla m_{k+1}(0) = \nabla f(x_{k+1})$$
$$\nabla m_{k+1}(-\alpha_k d_k) = \nabla f(x_k)$$
From the second equation above, we get
$$\nabla_{k+1}(-\alpha_k d_k) = \nabla f(x_{k+1}) - \alpha_k B_{k+1} d_k = \nabla f(x_k)$$
Which implies
$$\nabla(x_{k+1}) - \nabla f(x_k) = B_{k+1}(x_{k+1} - x_k)$$
This is the secant equation. We can use this equation to update $B_{k+1}$.



