\subsection{Lecture 11}
\subsubsection{Convergence of Line Search Methods}
\begin{definition}[Key Quantity]
    The key quantity is the angle between the search direction and the gradient. It is
    $$\cos \theta_k = \frac{\langle \nabla f(x_k), v_k \rangle}{\| \nabla f(x_k) \| \| v_k \|}$$
    We will use this together with Wolfe conditions to prove convergence.
\end{definition}
\begin{theorem}[Convergence of Line Search Methods]
    Let $x_0 \in \mathbb R^n \to \mathbb R$ be $C^1$-smooth, and $f$ be bounded below. Let $0 < c_1 < c_2 < 1$ and suppose that there exists a Lipschitz constant $L > 0$ such that $$ \| \nabla f(x) - \nabla f(y) \| \leq L \| x - y \|$$
    on an open set containing the level set $\{z: f(z) \leq f(x_0)\}$.

    Let $\{x_k\}$ be generated using descent directions $v_k$ and step lengths that satisfy the Wolfe conditions. Then we have
    $$\sum^\infty_{k=1} \cos^2 \theta_k \|\nabla f(x_k) \|^2 < \infty$$
\end{theorem}
\begin{proof}[Proof]
    In course notes.
\end{proof}
\begin{corollary}
    If there exists $\delta > 0$ satisfying $\cos^2 \theta_k \geq \delta > 0$ for all $k$, then 
    $$\lim_{k \to \infty} \nabla f(x_k) = 0$$
\end{corollary}

\subsubsection{Convergence Rate of Steepest Descent}
For exact line search, we have
$$x_{k+1} = x_k - \left(\frac{\| \nabla f(x_k)\|^2}{\langle A \nabla f(x_k), \nabla f(x_k) \rangle}\right) \nabla f(x_k)$$
If we plot out our iterations, we can see that there is a zig-zagging behaviour and the directions $x_{k+1} - x_k$ and $x_k - x_{k-1}$ are orthogonal.
\begin{theorem}[Zig Zagging of Steepest Descent]
    For the general steepest descent method with an exact step-size, we have
    $$(x_{k+1} - x_k)^T (x_k - x_{k-1}) = 0$$
    So they are orthogonal.
\end{theorem}
\begin{theorem}[Rate of Convergence of Steepest Descent]
    Suppose that $f: \mathbb R^n \to \mathbb R$ is $C^2$-smooth and the iterates $\{x_k\}$ generated by steepest descent with exact line search converge to $x^*$, that is, $\lim_{k \to \infty} x_k = x^*$. Suppose that $\nabla^2 f(x^*)$ is positive definite. Then for all large $k$, we have
    $$f(x_{k+1}) - f(x^*) \leq K (f(x_k) - f(x^*))$$ with 
    $$0 \leq K = \left(\frac{\lambda_1 - \lambda_n}{\lambda_1 + \lambda_n}\right)^2 < 1$$
    where $\lambda_1$ and $\lambda_n$ are the largest and smallest eigenvalues of $\nabla^2 f(x^*)$. 
\end{theorem}

\subsubsection{Convergence Rate of Newton's Method}

\begin{definition}[Comparing Algorithms]
    \textcolor{red}{upload later}
  \end{definition}
  \begin{definition}[$p$-th Order Convergence]
    If the sequence of residuals/errors $r_k \to 0$ and
    $$\lim_{k \to \infty} \frac{\|r_{k+1}\|}{\|r_k\|^p} \to r > 0$$
    then the order of convergence is called $p$-th order.

    If $p = 1, r < 1$, then this is called linear convergence. (For s.d, linear , r $\cong r$ (cond(Hessian at $x^*$)))
    
    If $p = 2$, then this is called quadratic convergence.

    If $$\lim_{k \to \infty} \frac{\|r_{k+1} \|}{\|r_k \|} \to 0$$ then this is called superlinear convergence.
  \end{definition}
  
  \begin{theorem}[Convergence of Newton's Method]
    Suppose that $f: \mathbb R^n \to \mathbb R$ is $C^2$-smooth, $\nabla^2 f(x^*)$ is positive definite, $\nabla f(x^*) = 0$ (also minor condition: Hessian is locally Lipschitz continuous, which in particular holds if $f$ is $C^3$-smooth). Consider the iterates
    \begin{align*}
        x_{k+1} &= x_k + t_k v_k \\
        &= x_k + t_k (\nabla^2 f(x_k))^{-1} (-\nabla f(x_k))
    \end{align*}
    where $t_k$ is the step size chosen to satisfy the Wolfe conditions (with $c_1 < \frac{1}{2}$). Then if the starting point $x_0$ is sufficiently close to $x^*$, then we have:
    \begin{enumerate}
        \item $t_k = 1$ satisfies the Wolfe conditions (pure/free Newton step).
        \item $x_k$ converges to $x^*$.
        \item If we choose $t_k = 1$, then we have quadratic convergence locally of the iterates, that is for some $r \geq 0$ and some $x_0$ close enough to the strict local minimum $x^*$, we have
        $$\|x_{k+1} - x^* \| \leq r \|x_k - x^*\|^2$$
        $$\| \nabla f(x_{k+1})\| \leq r \|\nabla f(x_k)\|^2$$
    \end{enumerate}
  \end{theorem}
  \begin{problem}
      $d_{SC}=-\nabla f(x_k),d_N=-\nabla^2f(x_k)^{-1}\nabla f(x_k) $
  \end{problem}
  
  %\begin{Problem}
      levenberg marquardt algorithm
  $-(\nabla^2 f(x_k)+\lambda I)^{-1}\nabla f(x_k)$ $\lambda >0$
  %\end{Problem}
  \begin{problem}
      This leads to trust region method ,
      follows by 5.8.1 Trust Region Methods Outline.
      note: Find $d_k$ is trust region subproblem
  \end{problem}

\subsubsection{Trust Region Strategy}
\begin{problem}[Trust Rigion Strategy]
  Construct a model for f at $x_k$, $f(x_k+\Delta x)\sim mq(\Delta x)=f(x_k)+\Delta x^T\nabla f(x_k)+\frac{1}{2}\Delta x^TB_k\Delta x$,
  where $B_k\simeq \nabla^2 f(x_k)$. COnstruct region where we trust the model. Region can be a ball,
  eg. $\Omega_k=\{\Delta x| ||\Delta x||\leq \delta_k^2\}$
\end{problem}
The difference in trust region strategy and line search strategy is that in line search strategy, we first choose the direction, then choose step size. In trust region, we first choose the step size, then we choose the direction.
\begin{definition}[Trust Region Strategy]
  In each iteration, we construct a model of $f$. That is, in each step we consider $m_k: \mathbb R^n \to \mathbb R$ that is a simple function that approximates $f$ well on some simple set $\Omega_k$ (the trust region) around our current approximation $x_k$. Then we find the new approximation $$\hat x = \text{argmin } m_k(x) \quad \text{$x \in \Omega_k$}$$
  A common model is the quadratic model $$m_k(x) = f(x_k) + \langle \nabla f(x_k), x - x_k \rangle + \frac{1}{2} \langle x - x_k, B_k(x - x_k) \rangle$$ where $B_k \approx \nabla^2 f(x_k)$ approximates the Hessian. If the values $f(\hat x)$ and $m_k(\hat x)$ are close, then we declare $x_{k+1} = \hat x$. Otherwise, we shrink the size of the trust region $\Omega_k$ and repeat the process.

  Usually $\Omega_k$ is a ball, ellipsoid, or a box around $x_k$.

  The main points are how to choose a model function $m_k$, and how to choose a trust region $\Omega_k$.
\end{definition}
\begin{theorem}[Trust Region Methods Outline]
    Outline:
    \begin{enumerate}
        \item Given $f: \mathbb R^n \to \mathbb R$, an iterate $x_k$, a local model function $$m_k(d) = f(x_k) + \langle \nabla f(x_k), d \rangle + \frac{1}{2} \langle B_kd, d \rangle$$
        and a trust region radius $\nabla_k > 0$
        \item Find $d_k \in \text{argmin} \{m_k(d) : \|d\| \leq \nabla_k\}$
        \item Use the relative decrease (actual/predicted decrease)
        $$\rho_k = \frac{f(x_k) - f(x_k + d_k)}{m_k(0) - m_k(d_k)}$$ to gauge the strength of the model in order to obtain the new trust region radius $\Delta_{k+1}$ and to decide whether to accept the new point as $x_{k+1} = x_k + d_k$ or use $x_{k+1} = x_k$.
    \end{enumerate}
\end{theorem}




\section{Convex set and function}
\subsection{Lecture 11-13}
\begin{definition}[Line Segment]
    Let $x,y \in \mathbb R^n$. The line segment joining $x$ and $y$ is the set
    $$[x,y] = \{\lambda x + (1 - \lambda)y : \lambda \in [0,1]\}$$
\end{definition}

\begin{definition}[Convex set]
    Set $S$ in $R^n$ in a convex set if for every $x, y \in S$ and for every  $0\leq \lambda \leq 1$ $$\lambda x+(1-\lambda)y\in S$$
    That is, the line segment $[x,y]$ is contained in $S$.
\end{definition}
\begin{problem}[Example of Convex Sets]
    The following are convex sets:
    \begin{enumerate}
        \item The empty set $\emptyset$, any single point $\{x_0\}$, and $\mathbb R^n$
        \item Lines in $\mathbb R^n$. For example, a line through the point $x_0$ in the direction $d$ $$L = \{x_0 + \alpha d: \alpha \in \mathbb R\}$$
        \item A halfspace in $\mathbb R^n$: $H = \{x: a^Tx \leq c\}$
        \item An open halfspace in $\mathbb R^n$: $H^o = \{x: a^Tx < c\}$
        \item A polyhedron in $\mathbb R^n$: $P = \{x: Ax \leq b\}$
        \item An open and closed ball in $\mathbb R^n$
        \item The set of $n \times n$ positive semi definite and positive definite matrices:
        $$S_+^n = \{A \in \mathbb R^{n \times n}: A \succeq 0\}, \quad S_{++}^n = \{A \in \mathbb R^{n \times n}: A \succ 0\}$$
    \end{enumerate}
\end{problem}
\begin{definition}[Convex combination]
    Let $S$ be a convex set and let $x_1,\ldots x_n \in S$. If $\lambda_1,\lambda_2,\ldots,\lambda_n$ are nonnegative numbers such that $$\sum_{i=1}^{n} \lambda_i = 1$$ then $$\sum^n_{i=1} \lambda_i x_i$$ is called a convex combination and is contained in $S$.
\end{definition}

\begin{lemma}
    $S$ is a convex set iff $S$ contains all its convex combination
\end{lemma}
\begin{definition}
    Given $S\in R^n$, the convex hull of $S$, $\text{conv }S$ is the smallest convex set containing $S$ and is equal to the set of all convex combinations of points in $S$.
\end{definition}

\begin{lemma}
    Suppose $S$ is a compact set, then $\text{conv }S$ is a compact set.
\end{lemma}
\begin{definition}[Affine Set]
    Let $S \subseteq \mathbb R^n$ be a subspace, and $a \in \mathbb R^n$. Then $L = a + S$ is an affine set (i.e. it is a translation of a subspace).

    Moreover, $\dim L = \dim S$. In addition, let $T \subseteq \mathbb R^n$. Then the affine hull, $\text{aff }T$, is the smallest affine set containing $T$. If $T$ is a convex set, then $\dim(T) = \dim(\text{aff }T)$.
\end{definition}
\begin{definition}[Convex Cone]
    $K \subseteq \mathbb R^n$ is a convex cone if the Minkowski sums satisfy
    \begin{enumerate}
        \item $\lambda K \subseteq K$ for all $\lambda \geq 0$
        \item $K + K \subseteq K$
    \end{enumerate}
\end{definition}
\begin{definition}[Hyperplane]
    For $a \in \mathbb R^n, b \in \mathbb R$, their hyperplane is defined as
    $$H = \{x \in \mathbb R^n: a^Tx = b\}$$
\end{definition}
\begin{definition}[Halfspace]
    For $a \in \mathbb R^n, b \in \mathbb R$, their halfspace is defined as
    $$H = \{x \in \mathbb R^n: a^Tx \leq b\}$$
\end{definition}
\begin{definition}[Polytope]
    A polytope(polyhedron set) is a finite intersection of halfspaces.
\end{definition}
\begin{definition}[Face]
    Let $C$ be a convex set. The convex set $F \subseteq C$ is a face of $C$, denoted as $F \unlhd C$ if $$x, y \in C, \quad z \in (x, y) \cap F \implies x, y \in F$$
    That is, for any two points $x,y \in C$, if a point $z$ lies in the open line segment between $x$ and $y$ (denoted $(x,y)$) also lies in $F$, then $x, y \in F$.

    If this is true, then $F$ is a face of $C$.
\end{definition}
\begin{theorem}[]
    If $C$ is a convex cone, then the faces of $C$ are convex cones.
\end{theorem}
\begin{proof}[Proof]
    Exercise.
\end{proof}
\begin{definition}[Non-negative Polar (Dual) Cone]
    The non-negative polar (dual) cone of a set $C$ is
    $$C^+ = C^* = \{\phi : \langle \phi, c \rangle \geq 0, \forall c \in C\}$$
\end{definition}
\begin{definition}[Conjugate Face]
    Let $F$ be a face of the convex cone $K$ denoted $F \unlhd K$. The conjugate face of $F$ is $$F^* = F^\bot \cap K^*$$
\end{definition}
\begin{definition}[Exposed Face]
    Let $F$ be a face of the convex cone $K$, $F \unlhd K$. Then $F$ is an exposed face if there exists $\phi \in K^*$ such that
    $$F = K \cap \phi^\bot$$
    where $\phi^\bot = \{\phi\}^\bot$.
\end{definition}
\begin{definition}[Extreme Point]
    Let $C \subseteq \mathbb R^n$. The point $x \in C$ is an extreme point of $C$ if
    $$x \in [y,z], \quad y, z \in C \implies x = y = z$$
    That is, there is no nontrivial convex combination with points in $C$.
\end{definition}
\begin{definition}[Convex Functions and Strictly Convex Functions]
    Let $f: C \to \mathbb R$ where $C \subseteq \mathbb R^n$ is a convex set. We say that $f$ is a convex function if for every $x, y \in C$ and for every $\lambda \in [0,1]$,
    $$f((1 - \lambda)x + \lambda y) \leq (1 - \lambda) f(x) + \lambda f(y)$$
    We say that $f$ is strictly convex if for every $x, y \in C, x \neq y$ and for every $\lambda \in [0,1]$,
    $$f((1 - \lambda)x + \lambda y) < (1 - \lambda) f(x) + \lambda f(y)$$
\end{definition}
\begin{theorem}[First Order Condition]
    Assume that $f(x)$ is continuously differentiable on a convex set $C \subseteq \mathbb R^n$. Then the function $f(x)$ is
    \begin{enumerate}
        \item convex if and only if for all $x, y \in C$, $$f(x) + \nabla f(x)^T (y - x) \leq f(y)$$
        \item strictly convex if and only if for all $x, y \in C$ with $x \neq y$,
        $$f(x) + \nabla f(x)^T (y - x) < f(y)$$
    \end{enumerate}
\end{theorem}
\begin{proof}[Proof]
    See past course notes
\end{proof}
\begin{corollary}
    Let $f(x)$ be a continuously differentiable convex function on an open convex set $C \subseteq \mathbb R^n$. If $x^* \in C$ is a critical point of $f$, then $x^*$ is a global minimizer of $f$ in $C$.
\end{corollary}
\begin{theorem}[Second Order Condition]
    Assume that $f(x)$ is twice continuously differentiable on an open convex set $C \subseteq \mathbb R^n$. Then the function $f(x)$ is convex if and only if $\nabla^2 f(x)$ is positive semidefinite for every $x \in C$.
\end{theorem}
\begin{proof}[Proof]
    See past course notes
\end{proof}
\begin{theorem}[]
    Let $C \subseteq \mathbb R^n$ be a convex set. Then any local minimizer (in $C$) of a convex function $f: C \to \mathbb R$ is also a global minimizer of $f$ on $C$. Furthermore, any local minimizer of a strictly convex function $f: C \to \mathbb R$ is the unique global minimizer of $f$ on $C$.
\end{theorem}