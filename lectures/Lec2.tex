% \documentclass[../main.tex]{subfiles}

% \begin{document}
\subsection{Lecture 2}
\begin{definition}[General NLO/NLP]
    A \textbf{Non-linear Optimization Problem} (NLP) is of the following form:
    $$\underbrace{p^*}_{\text{Optimal Value}} = \min\;\;\;\; \underbrace{f(x)}_{\text{Objective function}}$$
    s.t.
    \begin{align*}
        g(x) &= (g_i(x)) \leq 0 \in \mathbb R^m \\
        h(x) &= (h_j(x)) = 0 \in \mathbb R^p
    \end{align*}
\end{definition}
\begin{problem}[Example]
    $$\min (x_1 - 2)^2 + (x_2 - 1)^2$$
    s.t.
    \begin{align*}
        x_1^2 - x_2 &\leq 0 \tag{$g_1(x) \leq 0$}\\
        x_1 + x_2 - 2 &\leq 0 \tag{$g_2(x) \leq 0$}
    \end{align*}
\end{problem}

\begin{definition}[Contour]
    For $\alpha \in \mathbb R$, the \textbf{contour} of a function $f$ is $$C_\alpha = \{ x \in \mathbb R^n: f(x) = \alpha\}$$
\end{definition}
\begin{definition}[Feasible Set]
    The \textbf{feasible set} is $$F = \{x \in \mathbb R^n: g(x) \leq 0, h(x) = 0, x \in D\}$$ (Is $D$ the domain??)
\end{definition}
\begin{definition}[Gradient]
    The \textbf{gradient} of $f$ is $$\nabla f(x) = 
    \begin{pmatrix} \frac{\partial f(x)}{\partial x_1} \\ \vdots \\ \frac{\partial f(x)}{\partial x_n}\end{pmatrix}$$

    For the optimal solution $x^*$, we have
    $$\alpha \nabla f(x^*) = \lambda_1 \nabla g_1(x^*) + \lambda_2 \nabla g_2(x^*)$$ for some $\alpha, \lambda_1, \lambda_2 \in \mathbb R$.

    We will see later that we can choose $\alpha = 1$ and we need $\lambda_1 \geq 0, \lambda_2 \geq 0$.
\end{definition}
\begin{problem}[Max-cut Problem]
    Given a weighted graph $G = (\underbrace{V}_{\text{vertices}}, \underbrace{E}_{\text{edges}}, \underbrace{w}_{\text{weight}})$, a \textbf{cut} is $U \subseteq V, U \neq \emptyset$. The objective function $$\max \;\;\;\; \frac{1}{2} \sum_{\substack{i \in U, j \not \in U \\ (i,j) \in E}} w_{i,j}$$ maximizes the sum of edges in a cut.

    Formulating as an NLP, we introduce variables $x_i \in \{\pm 1\}, i = 1,\ldots, n$. Then the Max-cut problem (MC) is as follows:
    $$\max\;\;\;\; \frac{1}{2} \sum_{ij \in E} w_{ij}(1 - x_ix_j)$$ \textcolor{red}{Why 1/2}
    s.t. $$x_i \in \{\pm 1\}  \;\;\;\;(\text{equivalent to } x^2_i = 1)\;\;\;\; \forall i = 1,\ldots, n$$
    This works because $$1 - x_ix_j = \begin{cases} 0 & \text{if }x_i = x_j \;\;\;\; \text{($i,j$ in the same set, $U$ or $U^c$)} \\
    2 & \text{otherwise}
    \end{cases}$$
    MC is a \textbf{quadratically constrained quadratic program} (QOP) since each constraint $x_i \in \{-1, 1\}$ is equivalent to the quadratic constraint $x^2_i = 1$. Note that MC is an NP-hard problem.
\end{problem}
\section{Unconstrained Optimization}
\subsection{Lecture 2}
\begin{problem}[Simplest Case - No Constraints]
    Let $\Omega \subseteq \mathbb R^n$ be an open set. Assume $f$ is sufficiently smooth (differentiable) then the NLP with no constraints is $$\min_{x \in \Omega} \;\;\;\; f(x)$$
\end{problem}
\begin{definition}[Secant Line]
    A secant line is a line that connects two points on a function.
\end{definition}
\begin{theorem}[Chain Rule (2 dimensions)]
    Let $f: \mathbb R^2 \rightarrow \mathbb R$, and two other functions $x(t): \mathbb R \rightarrow \mathbb R$ and $y(t): \mathbb R \rightarrow \mathbb R$. Let $\phi(t) = f(x(t), y(t))$. The chain rule then states
    $$\frac{d\phi}{dt} = \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt}$$
\end{theorem}
\begin{problem}[Example of Chain Rule in 2 Dimensions]
    Let $f(x,y) = x^2 + y^2$. We want to find the rate of change of $f$ along a curve defined by $x(t) = t$ and $y(t) = 2t$. The partial derivatives of $f$ are: $$\frac{\partial f}{\partial x} = 2x, \quad \frac{\partial f}{\partial y} = 2y$$ The derivatives of $x(t)$ and $y(t)$ are $$\frac{dx}{dt} = 1, \quad \frac{dy}{dt} = 2$$ Then we get
    \begin{align*}
        \frac{d}{dt} f(x(t), y(t)) &= \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt} \\
        &= 2x \cdot 1 + 2y \cdot 2 \\
        &= 2(t) \cdot 1 + 2(2t) \cdot 2 \\
        &= 10t
    \end{align*}
\end{problem}
\begin{corollary}
    Let $f: (a,b) \rightarrow \mathbb R$
    \begin{enumerate}
        \item (Necessity)If $\bar x$ is a \textbf{local minimizer} of $f$ on $(a,b)$, then $f'(\bar x) = 0$ and $f''(\bar x) \geq 0$.
        \item (sufficient)If $f(\bar x) = 0, f''(\bar x) > 0$ then $\bar x$ is a \textbf{strict local minimizer} of $f$.
    \end{enumerate}

    
\end{corollary}
\begin{proof}
    \begin{align*}
        \text{Since $\bar{x}$ is the local minimizer of f, given small $|\Delta x|$},
        \\ f(\bar{x}+\Delta x)-f(\bar{x}) &\geq 0 \\
        f(\bar{x}+\Delta x) & = f(\bar{x})+f'(\bar{x})*\Delta x+o(|\Delta x|) \\
        f'(\bar{x})*\Delta x+o(|\Delta x|) &\geq 0 \\
        \frac{f'(\bar{x})}{|\Delta x|}*\Delta x+\frac{o(|\Delta x|)}{|\Delta x|} &\geq 0 \\
        \end{align*}
    Since this holds for all positive and negative $\Delta x$. Thus $f'(x)=0$
        For the second order proof, we used taylor thm to construct the contradiction
\end{proof}
\begin{definition}[Hessian]
    The \textbf{Hessian} of $f$ at $x = \begin{pmatrix}
        x_1 \\ \vdots \\ x_n
    \end{pmatrix}$ is the matrix
    $$\nabla^2 f(x) = \begin{bmatrix}
        \frac{\partial f(x)}{\partial x^2_1} & \frac{\partial f(x)}{\partial x_1 \partial x_2} & \cdots & \frac{\partial f(x)}{\partial x_1 \partial x_n} \\
        \frac{\partial f(x)}{\partial x_2 \partial x_1} & \frac{\partial f(x)}{\partial x_2^2} & \cdots & \frac{\partial f(x)}{\partial x_2 \partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial f(x)}{\partial x_n \partial x_1} & \frac{\partial f(x)}{\partial x_n \partial x_2} & \cdots & \frac{\partial f(x)}{\partial x_n^2} \\
    \end{bmatrix}$$
\end{definition}

\begin{lemma}
    Let $v \in \mathbb R^n$. Then $$v = 0 \iff \langle v,d \rangle = 0, \quad \forall d \in \mathbb R^n$$
\end{lemma}

% \end{document}