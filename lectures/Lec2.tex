% \documentclass[../main.tex]{subfiles}

% \begin{document}
\subsection{Lecture 2}
\begin{definition}[General NLO/NLP]
    A \textbf{Non-linear Optimization Problem} (NLP) is of the following form:
    $$\underbrace{p^*}_{\text{Optimal Value}} = \min\;\;\;\; \underbrace{f(x)}_{\text{Objective function}}$$
    s.t.
    \begin{align*}
        g(x) &= (g_i(x)) \leq 0 \in \mathbb R^m \\
        h(x) &= (h_j(x)) = 0 \in \mathbb R^p
    \end{align*}
\end{definition}
\begin{problem}[Example]
    $$\min (x_1 - 2)^2 + (x_2 - 1)^2$$
    s.t.
    \begin{align*}
        x_1^2 - x_2 &\leq 0 \tag{$g_1(x) \leq 0$}\\
        x_1 + x_2 - 2 &\leq 0 \tag{$g_2(x) \leq 0$}
    \end{align*}
\end{problem}

\begin{definition}[Contour]
    For $\alpha \in \mathbb R$, the \textbf{contour} of a function $f$ is $$C_\alpha = \{ x \in \mathbb R^n: f(x) = \alpha\}$$
\end{definition}
\begin{definition}[Feasible Set]
    The \textbf{feasible set} is $$F = \{x \in \mathbb R^n: g(x) \leq 0, h(x) = 0, x \in D\}$$ (Is $D$ the domain??)
\end{definition}
\begin{definition}[Gradient]
    The \textbf{gradient} of $f$ is $$\nabla f(x) = 
    \begin{pmatrix} \frac{\partial f(x)}{\partial x_1} \\ \vdots \\ \frac{\partial f(x)}{\partial x_n}\end{pmatrix}$$

    For the optimal solution $x^*$, we have
    $$\alpha \nabla f(x^*) = \lambda_1 \nabla g_1(x^*) + \lambda_2 \nabla g_2(x^*)$$ for some $\alpha, \lambda_1, \lambda_2 \in \mathbb R$.

    We will see later that we can choose $\alpha = 1$ and we need $\lambda_1 \geq 0, \lambda_2 \geq 0$.
\end{definition}
\begin{problem}[Max-cut Problem]
    Given a weighted graph $G = (\underbrace{V}_{\text{vertices}}, \underbrace{E}_{\text{edges}}, \underbrace{w}_{\text{weight}})$, a \textbf{cut} is $U \subseteq V, U \neq \emptyset$. The objective function $$\max \;\;\;\; \frac{1}{2} \sum_{\substack{i \in U, j \not \in U \\ (i,j) \in E}} w_{i,j}$$ maximizes the sum of edges in a cut.

    Formulating as an NLP, we introduce variables $x_i \in \{\pm 1\}, i = 1,\ldots, n$. Then the Max-cut problem (MC) is as follows:
    $$\max\;\;\;\; \frac{1}{2} \sum_{ij \in E} w_{ij}(1 - x_ix_j)$$ \textcolor{red}{Why 1/2}
    s.t. $$x_i \in \{\pm 1\}  \;\;\;\;(\text{equivalent to } x^2_i = 1)\;\;\;\; \forall i = 1,\ldots, n$$
    This works because $$1 - x_ix_j = \begin{cases} 0 & \text{if }x_i = x_j \;\;\;\; \text{($i,j$ in the same set, $U$ or $U^c$)} \\
    2 & \text{otherwise}
    \end{cases}$$
    MC is a \textbf{quadratically constrained quadratic program} (QOP) since each constraint $x_i \in \{-1, 1\}$ is equivalent to the quadratic constraint $x^2_i = 1$. Note that MC is an NP-hard problem.
\end{problem}
\section{Unconstrained Optimization}
\subsection{Lecture 2}
\begin{problem}[Simplest Case - No Constraints]
    Let $\Omega \subseteq \mathbb R^n$ be an open set. Assume $f$ is sufficiently smooth (differentiable) then the NLP with no constraints is $$\min_{x \in \Omega} \;\;\;\; f(x)$$
\end{problem}
\begin{theorem}[Taylor's Theorem on the real line]
    Let $f: (a, b) \rightarrow \mathbb R$, and $\bar{x}, x \in (a,b)$. Then the Taylor's series centered at $\bar x$ (approximation near $\bar x$) is $$f(x) = f(\bar x) + f'(\bar x)(x - \bar x) + \frac{f''(z)}{2}(x - \bar x)^2$$
    where $z$ is between $x$ and $\bar x$ that gives the largest value of $f''(z)$. The term $\frac{f''(z)}{2}(x - \bar x)^2$ is the \textbf{error term}

    Equivalently,
    $$f(\bar x + \Delta x) = \underbrace{f(x) + f'(x) \Delta x}_{\text{Linear approximation}} + o(|\Delta x|)(\text{little O})$$
    This formula emphasizes its use in approximating changes in $f$ for small changes in $x$, denoted $\Delta x$. $o(|\Delta x|)$, the error term, means that the error goes to 0 faster than $|\Delta x|$ as $\Delta x$ goes to 0. Therefore, this is saying that the linear approximation becomes more and more accurate for smaller $\Delta x$.
\end{theorem}
\begin{definition}[Secant Line]
    A secant line is a line that connects two points on a function.
\end{definition}
\begin{theorem}[Chain Rule (2 dimensions)]
    Let $f: \mathbb R^2 \rightarrow \mathbb R$, and two other functions $x(t): \mathbb R \rightarrow \mathbb R$ and $y(t): \mathbb R \rightarrow \mathbb R$. Let $\phi(t) = f(x(t), y(t))$. The chain rule then states
    $$\frac{d\phi}{dt} = \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt}$$
\end{theorem}
\begin{problem}[Example of Chain Rule in 2 Dimensions]
    Let $f(x,y) = x^2 + y^2$. We want to find the rate of change of $f$ along a curve defined by $x(t) = t$ and $y(t) = 2t$. The partial derivatives of $f$ are: $$\frac{\partial f}{\partial x} = 2x, \quad \frac{\partial f}{\partial y} = 2y$$ The derivatives of $x(t)$ and $y(t)$ are $$\frac{dx}{dt} = 1, \quad \frac{dy}{dt} = 2$$ Then we get
    \begin{align*}
        \frac{d}{dt} f(x(t), y(t)) &= \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt} \\
        &= 2x \cdot 1 + 2y \cdot 2 \\
        &= 2(t) \cdot 1 + 2(2t) \cdot 2 \\
        &= 10t
    \end{align*}
\end{problem}
\begin{lemma}[Directional Derivative]
    Let $f: \mathbb R^n \rightarrow \mathbb R, \bar x, d \in \mathbb R^n$ where $d$ is the direction. We define
    $$\phi (\epsilon) = f(\bar x + \epsilon d): \mathbb R \rightarrow \mathbb R$$ the value of the function $f$ at a point that is displaced from $\bar x$ by a distance of $\epsilon$ in the direction $d$.
    Then the \textbf{directional derivative}, denoted $f'(x; d)$ of $f$ at $x$ at the direction $d$ is
    $$f'(x; d) = \phi'(0) = \lim_{\epsilon \rightarrow 0} \frac{f(x + \epsilon d) - f(x)}{\epsilon} = \nabla f(x)^T d$$
\end{lemma}
\begin{definition}[Directional Derivative (Different notation)]
    The directional derivative of $f$ at a point $\bar x, \in \mathbb R^n$ in the direction $d$ is $$f'(\bar x; d) = \frac{d}{ds} f(\bar x + sd) \bigg|_{s = 0}$$
\end{definition}
\begin{theorem}[]
    If $f$ is differentiable at $\bar x$, then $$f'(\bar x; d) = \nabla f(\bar x)^T d$$
\end{theorem}
\begin{proof}[Proof]
    We only prove in the case where $\bar x = (a, b) \in \mathbb R^2$.
    \begin{align*}
        f'(\bar x; d) &= \frac{d}{ds} f(\bar x + sd) \bigg|_{s = 0} \\
        &= \frac{d}{ds} f(\underbrace{a + sd_1}_{x}, \underbrace{b + sd_2}_{y}) \bigg|_{s = 0} \\
        &= \left[\frac{\partial f}{\partial x} \frac{dx}{ds} + \frac{\partial f}{\partial y}\frac{dy}{ds}\right]_{s=0} \tag*{Chain rule}\\
        &= \left[\frac{\partial}{\partial x}f(a + sd_1, b + sd_2) \cdot d_1 + \frac{\partial}{\partial y}f(a + sd_1, b + sd_2) \cdot d_2\right]_{s=0} \\
        &= \frac{\partial f}{\partial x} (a,b) \cdot d_1 + \frac{\partial f}{\partial y}(a,b) \cdot d_2 \\
        &= \left(\frac{\partial f}{\partial x}(a,b), \frac{\partial f}{\partial y} (a,b)\right) \cdot (d_1, d_2) \\
        &= \nabla f(a,b) \cdot d
    \end{align*}
\end{proof}
\begin{problem}
    Let $f(x,y,z) = x^2z + y^3z^2 - xyz$ with $d = \begin{pmatrix}
        -1 \\ 0 \\ 3
    \end{pmatrix}$ Then the \textbf{directional derivative} in the direction $d$ is
    $$\nabla f(x,y,z)^T d = \begin{pmatrix}
        2xz - yz \\
        3y^2z^2 - xz \\
        x^2 + 2y^3z - xy
    \end{pmatrix}^{T} \begin{pmatrix}
        -1 \\ 0 \\ 3
    \end{pmatrix} = -2xz + yz + 3x^2 + 6y^3z - 3xy$$
\end{problem}
\begin{corollary}
    Let $f: (a,b) \rightarrow \mathbb R$
    \begin{enumerate}
        \item If $\bar x$ is a \textbf{local minimizer} of $f$ on $(a,b)$, then $f'(\bar x) = 0$ and $f''(\bar x) \geq 0$.
        \item If $f(\bar x) = 0, f''(\bar x) > 0$ then $\bar x$ is a \textbf{strict local minimizer} of $f$.
    \end{enumerate}
\end{corollary}
\begin{definition}[Hessian]
    The \textbf{Hessian} of $f$ at $x = \begin{pmatrix}
        x_1 \\ \vdots \\ x_n
    \end{pmatrix}$ is the matrix
    $$\nabla^2 f(x) = \begin{bmatrix}
        \frac{\partial f(x)}{\partial x^2_1} & \frac{\partial f(x)}{\partial x_1 \partial x_2} & \cdots & \frac{\partial f(x)}{\partial x_1 \partial x_n} \\
        \frac{\partial f(x)}{\partial x_2 \partial x_1} & \frac{\partial f(x)}{\partial x_2^2} & \cdots & \frac{\partial f(x)}{\partial x_2 \partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial f(x)}{\partial x_n \partial x_1} & \frac{\partial f(x)}{\partial x_n \partial x_2} & \cdots & \frac{\partial f(x)}{\partial x_n^2} \\
    \end{bmatrix}$$
\end{definition}
\begin{theorem}[Multivariate Taylor]
    Consider a $C^2$-smooth function $f: U \rightarrow \mathbb R$ on an open set $U \subset \mathbb R^n$. If $\bar x$ and $x$ are such that the segment $[\bar x, x] := \{\bar x + t(x - \bar x): t \in [0,1]\}$ is contained in $U$, then the Taylor series expansion of $f$ centered around $\bar x$ is
    $$f(x) = f(\bar x) + \langle \nabla f(\bar x), x - \bar x \rangle + \frac{1}{2} \langle \nabla^2 f(z) (x - \bar x), (x - \bar x) \rangle$$
    where $z$ is between $x$ and $\bar x$.
\end{theorem}
\begin{lemma}
    Let $v \in \mathbb R^n$. Then $$v = 0 \iff \langle v,d \rangle = 0, \quad \forall d \in \mathbb R^n$$
\end{lemma}

% \end{document}