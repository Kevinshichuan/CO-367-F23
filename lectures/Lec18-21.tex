\subsection{Lectures 18-21}
\subsubsection{Constraint Qualification}
\begin{definition}[Constraint Qualification]
    A condition on the constraints that define the feasible set $\Omega$ is called a constraint qualification (CQ) at a feasible point $\bar x$ if whenever $\bar x$ is a local minimizer then the KKT optimality conditions hold at $\bar x$.
\end{definition}
\begin{lemma}
    Let $a_i, a_j \in \mathbb R^n$ for all $i \in I, j \in J$ where $I$ is the index set of inequality constraints and $J$ is the index set of equality constraints. Let
    $$Q = \left\{v \in \mathbb R^n : \substack{\langle a_i, v \rangle \geq 0, \quad \forall i \in I \\ \langle a_j, v \rangle = 0 , \quad \forall j \in J}\right\}$$
    Then
    $$Q = \{a_j\}^\perp_{j \in J} \cap \{a_i\}^+_{i \in I}$$
    and the nonnegative polar is
    $$Q^+ = \text{span}\{a_j\}_{j \in J} + \text{cone}\{a_i\}_{i \in I}$$
\end{lemma}
\begin{definition}[Weakest Constraint Qualification (WCQ)]
    We say that the weakest constraint qualification holds at a feasible point $\bar x$ if $$T_\Omega(\bar x) = \mathcal L(\bar x)$$
    That is, the tangent cone is equal to the linearizing cone.
\end{definition}
\begin{theorem}[]
    Suppose $\bar x$ is a local minimizer and WCQ holds at $\bar x$. Then the KKT conditions hold at $\bar x$.
\end{theorem}
\begin{theorem}[]
    Let $\bar x$ be a feasible point. Then the following are true.
    $$T_\Omega(\bar x) \subseteq L(\bar x)$$
    If LICQ holds at $\bar x$, then $$T_\Omega(\bar x) = L(\bar x)$$
\end{theorem}
\begin{theorem}[]
    If $x^*$ is a local minimizer of an NLP, then for all $d \in T_\Omega(x^*)$, the directional derivative is nonnegative: $$\nabla f(x^*)^Td \geq 0$$
\end{theorem}
\begin{definition}[Slater Point, Slater Constraint Qualification (SCQ)]
    Consider a convex program and suppose there exists $\bar x$ such that $g_i(\bar x) < 0$ for all $i \in I$ and $g_j(\bar x) = 0$ for all $j \in J$. Then $\bar x$ is called a slater point. The above is called the Slater constraint qualification (SCQ). Then for every $x \in \Omega$, $$T_\Omega(x) = L(x)$$
\end{definition}
\begin{theorem}[]
    Let $f, g_1,\ldots,g_m$ be continuously differentiable. Assume that the following KKT conditions hold at $\bar x \in \Omega$ for a convex program:

    There exists $\lambda_i \geq 0$ for all $i \in A(\bar x) \cap I$ and $\mu_j \in \mathbb R$ for all $j \in J$ such that $$\nabla f(\bar x) + \sum_{i \in A(\bar x) \cap I} \lambda_i \nabla g_i(\bar x) + \sum_{j \in J} \mu_j \nabla g_j(\bar x) = 0$$
    Then $\bar x$ is a global minimizer.
\end{theorem}
\begin{proof}[Proof]
    Let $\bar x \in \Omega$ satisfy $$\nabla f(\bar x) + \sum_{i \in A(\bar x) \cap I} \lambda_i \nabla g_i(\bar x) + \sum_{j \in J} \mu_j \nabla g_j(\bar x) = 0$$ and let $x \in \Omega$ be an arbitrary feasible point. We want to show that $f(x) \geq f(\bar x)$. Since $\Omega$ is convex, we have that each $g_i$ is convex. Then by \hyperref[thm:first-order-convex]{First Order Convexity Conditions}, we get 
    $$g_i(x) - g_i(\bar x) \geq \nabla g_i(\bar x)^T (x - \bar x)$$ This implies that $$\nabla g_i(\bar x)^T (x - \bar x) \leq 0$$
    Let $j \in J$. Then $g_j(x) = a_j^T - b_j = 0$ and $g_j(\bar x) = a_j^T \bar x - b_j = 0$. This gives $$a_j^T (x - \bar x) = 0$$ Since $\bar x$ satisfies our initial condition, we have 
    $$\nabla f(\bar x)^T (x - \bar x) = \underbrace{-\sum_{i \in A(\bar x) \cap I} \lambda_i \nabla g_i(\bar x)^T (x - \bar x)}_{\geq 0} - \underbrace{\sum_{j \in J} \mu_j \nabla g_j(\bar x)^T (x - \bar x)}_{= 0} \geq 0$$
    Since $f$ is convex, we get $$f(x) - f(\bar x) \geq \nabla f(\bar x)^T (x - \bar x) \geq 0$$ So $\bar x$ is a global minimizer.
\end{proof}
\begin{theorem}[]
    Consider a convex problem with a feasible region $\Omega$ such that $f, g_1,\ldots,g_m$ are continuously differentiable. Suppose $\bar x \in \Omega$ and there exists Lagrangian multipliers $\lambda_1,\ldots, \lambda_m \geq 0$ and $\mu_1,\ldots,\mu_p \in \mathbb R$ such that $$\nabla f(\bar x) + \sum^m_{i=1} \lambda_i \nabla g_i(\bar x) + \sum^p_{j=1} \mu_j \nabla g_j(\bar x) = 0$$ and $$\lambda_i g_i(\bar x) = 0$$ for all $i = 1,\ldots,m = I$. Then $\bar x$ is an optimal solution.
\end{theorem}
\begin{proof}[Proof]
    Omitted
\end{proof}
\begin{theorem}[]
    Consider an NLP with the feasible region $\Omega$, where all active constraints at $\bar x$ are affine. Then $$T_\Omega(\bar x) = L(\bar x)$$
\end{theorem}
\begin{proof}[Proof]
    This is assignment 6 question 3
\end{proof}
\subsubsection{Second Order Conditions}
\begin{definition}[Critical Cone]
    The critical cone at $\bar x \in \Omega$ for the minimization problem 
    \begin{align*}
        \min \quad &f(x) \\
        \text{s.t.} \quad &g_i(x) \leq 0, \quad i \in I \\
        &g_j(x) = 0, \quad j \in J
    \end{align*}
    is the set $$C(\bar x, \lambda^*, \mu^*) = \{d \in \mathcal L(\bar x): \nabla c_i(\bar x)^T d = 0, \quad \forall i \in A(\bar x) \text{ with } \lambda_i > 0\}$$
\end{definition}
\begin{theorem}[Second Order Optimality Conditions]
    (Necessary condition) Let $x^*$ be a local minimizer and LICQ holds at $x^*$. Then $x^*$ is a KKT point with unique lagrange multipliers $\lambda^*, \mu^*$ such that $$\langle \nabla^2_{x, x} L(x^*, \lambda^*, \mu^*)d, d \rangle \geq 0, \quad \forall d \in C(x^*, \lambda^*, \mu^*)$$
    (Sufficient Conditions) If $x^*$ is a KKT point with multipliers $\lambda^*, \mu^*$ such that $$\langle \nabla^2_{x, x} L(x^*, \lambda^*, \mu^*)d, d \rangle > 0, \quad \forall 0 \neq d \in C(x^*, \lambda^*, \mu^*)$$ then $x^*$ is a strict local minimizer. 
\end{theorem}
\begin{example}
    Consider 
    \begin{align*}
        \min_{x \in \mathbb R^2} \quad &x_1^2 + x_2^2 \\
        \text{s.t.} \quad &x_1^2 + x_2^2 - 2\leq 0 \\
    \end{align*}
    The Lagrangian is $$L(x, \lambda) = x_1 + x_2 + \lambda(x_1^2 + x_2^2 - 2)$$
    With gradient
    $$\nabla_x L(x, \lambda) = \begin{pmatrix}
        2\lambda x_1 + 1 \\
        2\lambda x_2 + 1
    \end{pmatrix} $$
    The KKT conditions are 
    \begin{align*}
        \nabla_x L(x, \lambda) = \begin{pmatrix}
            2\lambda x_1 + 1 \\
            2\lambda x_2 + 1
        \end{pmatrix} &= 0 \tag{1}\\
        \lambda &\geq 0 \tag{2}\\
        x_1^2 + x_2^2 - 2 &\leq 0 \tag{3}\\
        \lambda(x_1^2 + x_2^2 - 2) &= 0 \tag{4}
    \end{align*}
    From (4), either $\lambda = 0$ or $x_1^2 + x_2^2 - 2 = 0$. Notice that $\lambda \neq 0$ since $\lambda = 0$ does not satisfy equation (1). So we must have $\lambda > 1$ and $x_1^2 + x_2^2 - 2 = 0$. 
    
    \bigskip 
    (1) implies $x_1 = - \frac{1}{2\lambda_1}, x_2 = - \frac{1}{2\lambda_2}$. Substituting this into $x_1^2 + x_2^2 - 2 = 0$ gives $\lambda = \frac{1}{2}$ which implies $x = (x_1,x_2) = (-1,-1)$. 

    \bigskip
    Now, the Hessian for the Lagrangian is $$\nabla^2_{x, x} L(x, \lambda) = \begin{bmatrix}
        2\lambda & 0 \\
        0 & 2\lambda
    \end{bmatrix} = \begin{bmatrix}
        1 & 0 \\
        0 & 1
    \end{bmatrix} \succ 0$$
    Since the Hessian is positive definite for every $d$, we have $\langle \nabla^2_{x, x} L(x, \lambda)d, d \rangle > 0$ for all $d \in C(x, \lambda)$. Thus by second order optimality conditions, $x$ is a strict local minimizer.
\end{example}

\subsubsection{Duality}
Consider the problem
\begin{align*}
    \min_{x \in D} \quad &f(x) \\
    \mathrm{s.t.} \quad &g_i(x) \leq 0, \quad i \in I = \{1,\ldots,m\} \\
    &h_j(x) = 0, \quad j \in J = \{1,\ldots,p\}
\end{align*}
where $f, g_i, h_j$ are defined on $D$. We call this the primal problem. The Lagrangian is
$$L(x, \lambda, \mu) = f(x) + \sum_{i \in I} \lambda_i g_i(x) + \sum_{j \in J} \mu_j h_j(x)$$
where $\lambda \in \mathbb R^m_+, \mu \in \mathbb R^p$.

\bigskip
The dual problem is
\begin{align*}
    \max \quad &q(\lambda, \mu) \\
    \text{s.t.} \quad &(\lambda, \mu) \in \mathrm{dom}(q)
\end{align*}
where $$q(\lambda, \mu) = \min_{x \in D} L(x, \lambda, \mu)$$ $$\mathrm{dom}(q) = \{(\lambda, \mu) \in \mathbb R^m_+ \times \mathbb R^p: q(\lambda, \mu) > -\infty\}$$

\begin{example}[Lagrangian Dual of an LP]
    Consider the primal 
    \begin{align*}
        \min \quad &c^Tx \\
        \text{s.t.} \quad &Ax = b
    \end{align*}
    The Lagrangian is $$L(x, \lambda) = c^Tx + \lambda^T(Ax - b) = (c^T + \lambda^TA)x - \lambda^Tb$$
    The dual function is then $$q(\lambda) =  -\lambda^Tb + \min_{x \in \mathbb R^n} (c^T + \lambda^TA)x$$
    Then $$q(\lambda) = \begin{cases}
        -\lambda^Tb, &\text{if } c^T + \lambda^TA = 0 \\
        -\infty, &\text{otherwise}
    \end{cases}$$ Therefore, the dual problem is 
    \begin{align*}
        \max \quad &-\lambda^Tb \\
        \text{s.t.} \quad &c^T + \lambda^TA = 0 \\
        &\lambda \geq 0
    \end{align*}
\end{example}
\begin{theorem}[Weak Duality]
    Consider the LP
    \begin{align*}
        \min_{x \in D} \quad &f(x) \\
        \mathrm{s.t.} \quad &g_i(x) \leq 0, \quad i \in I = \{1,\ldots,m\} \\
        &h_j(x) = 0, \quad j \in J = \{1,\ldots,p\}
    \end{align*}
    and its dual problem. Let $p^*$ and $d^*$ denote the optimal primal and dual objective values respectively. Then $$d^* \leq p^*$$
    The difference $p^* - d^*$ is called the \textbf{duality gap}. If $p^* = d^*$, then we say that \textbf{strong duality} holds.
\end{theorem}
\begin{definition}[Strong Duality]
    Let $p^*$ and $d^*$ denote the optimal primal and dual objective values respectively. If $p^* = d^*$ and $d^*$ is attained, then we say that \textbf{strong duality} holds.
\end{definition}
\begin{theorem}[Strong Duality]
    Consider the primal convex program $$p^* = min\{f(x): g(x) \leq 0, x \in \Omega\}$$ Suppose that the optimal value $p^*$ is finite and the Slater condition holds, i.e. there exists a feasible point $$\bar x \in \Omega, g(\bar x) < 0$$ Then strong duality holds. So there is zero duality gap and the dual is attained with $\lambda^* \geq 0$, $$p^* = d^* = \min_{x \in \Omega} L(x, \lambda^*)$$
    where the Lagrangian is $$L(x, \lambda) = f(x) + \langle \lambda^*, g(x) \rangle$$ Moreover, suppose that $p^*$ is attained, i.e. $p^* = f(x^*)$ with $x^*$ feasible in the primal. Then complementary slackness holds, so $$\langle \lambda^*, g(x^*) \rangle = 0$$ In addition, complementary slackness and strong duality imply that $x^*$ is a global minimum.
\end{theorem}
\begin{theorem}[Primal-Dual Sufficient Optimality Conditions]
    Suppose we have a minimization problem (P) and the following hold:
    \begin{itemize}
        \item $\bar x$ is a feasible solution for (P)
        \item $f(x)$ and $-c_i(x)$ are all convex for all inequality constraints, $i \in I$
        \item $c_j(x)$ are affine for all equality constraints, $j \in J$
        \item $(\bar x, \bar \lambda, \bar \mu)$ is a KKT point for (P)
    \end{itemize}
    Then $(\bar \lambda, \bar mu)$ solves the associated dual problem (D) with value $q(\bar \lambda, \bar \mu) = f(\bar x)$ and $\bar x$ solves the primal.
\end{theorem}