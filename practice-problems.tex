\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\input{preamble.tex}
\title{CO 367 Practice Problems for Final}

\begin{document}
\maketitle
\section{Unconstrained Optimality Conditions}
Find an example showing:
\begin{enumerate}
    \item The second order necessary optimality conditions for unconstrained optimization are not sufficient.
    
    \textbf{Solution.} We find a function $f(x)$ such that its gradient at $\bar x$ is 0 and hessian at $\bar x$ is positive semidefinite, but it is not a local minimizer. Let $f(x,y) = x^3 + y^3$. Then $$\nabla f(x,y) = \begin{bmatrix} 3x^2 \\ 3y^2 \end{bmatrix}, \quad \nabla^2 f(x,y) = \begin{bmatrix} 6x & 0 \\ 0 & 6y \end{bmatrix}$$ At (0,0), $\nabla f(0,0) = (0,0)^T$ and $\nabla^2 f(0,0) = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix}$. So this satisfies the necessary optimality conditions, but it is a saddle point.
    \item The second order sufficient optimality conditions for unconstrained optimization are not necessary.
    
    \textbf{Solution.} We find a function $f(x)$ such that $\bar x$ is a strict local minimizer, but the hessian at $\bar x$ is not positive definite. Let $f(x) = x^4 + y^4$. Then $$\nabla f(x,y) = \begin{bmatrix} 4x^3 \\ 4y^3 \end{bmatrix}, \quad \nabla^2 f(x,y) = \begin{bmatrix} 12x^2 & 0 \\ 0 & 12y^2 \end{bmatrix}$$ At (0,0), $\nabla f(0,0) = (0,0)^T$ and $\nabla^2 f(0,0) = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} \succeq 0$. So the hessian is not positive definite.  
\end{enumerate}
\section{Constrained Optimality Conditions}
Find an explicit optimal solution to the following problems:
\begin{enumerate}
    \item $\min\{c^Tx : Ax = b, x \in \mathbb R\}$
    
    \textbf{Solution.} We use second order necessary condition: Let $x^*$ be a local minimizer and LICQ holds at $x^*$. Then $x^*$ is a KKT point with unique lagrange multipliers $\lambda^*, \mu^*$ such that $$\langle \nabla^2_{x, x} L(x^*, \lambda^*, \mu^*)d, d \rangle \geq 0, \quad \forall d \in C(x^*, \lambda^*, \mu^*)$$

    \bigskip
    Suppose we have a local minimizer $\bar x$ that is a KKT point and KKT pair $(\bar x, \mu)$. We derive it using the necessary conditions. The lagrangian is $$L(\bar x, \mu) = c^T\bar x + \mu^T(A\bar x - b)$$
    Then $$\nabla_x L(\bar x, \mu) = c + A^T\mu, \quad \nabla^2_{x, x} L(\bar x, \mu) = 0$$ The KKT conditions are 
    \begin{align*}
        \nabla_x L(\bar x, \mu) = c + A^T\mu &= 0 \\
        A\bar x - b &= 0
    \end{align*}
    $\langle \nabla^2_{x, x} L(x^*, \lambda^*, \mu^*)d, d \rangle = 0$ for all $d$, so we just need $\bar x$ that satisfies the KKT conditions. So the local minimizer is $\bar x = A^{-1}b$ along with $\mu = -A^{-T}c$. \textcolor{red}{idk}
    \item $\min\{c^Tx : e^Tx = 1, x \geq 0\}$
    
    \textbf{Solution.}
\end{enumerate}
\section{Constrained Equivalent Optimality Conditions}
Is the following claim true or false? Justify your answer.

\bigskip
Consider the following constrained optimization problem:
\begin{align*}
    \min \quad &f(x) \\
    \text{s.t.} \quad &c_i(x) = 0, \quad i \in \mathcal E 
\end{align*}
Assume LICQ holds for this problem. We consider the following equivalent problem (equivalent geometrically)
\begin{align*}
    \min \quad &f(x) \\
    \text{s.t.} \quad &c_i^2(x) = 0, \quad i \in \mathcal E
\end{align*}
Let $x^*$ be a KKT point of the above problem, then $x^*$ satisfies 
\begin{align*}
    0 &= \nabla f(x^*) + 2 \sum_{i \in \mathcal E} \lambda^* c_i(x^*) \nabla c_i(x^*) \\
    0 &= c_i(x^*) \quad \forall i \in \mathcal E
\end{align*}
where $\lambda_i^*$ are the corresponding Lagrangian multipliers. By rearrangement, we have $\nabla f(x^*) = 0$. This implies for equality constrained optimization problems, we can get the equivalent first order necessary optimal condition as for the unconstrained optimization problem.

\textbf{Solution.}

\section{Duality}
Find the dual problem of the following problem and the dual of the dual problem.
\begin{align*}
    \min_{x \in \mathbb R^n} \quad & x^TAx + 2b^T x \\
    \text{s.t.} \quad & \|x\|_2 \leq 1
\end{align*}
where $A \in \mathcal S^n, b \in \mathbb R^n$ (Use the generalized inverse to derive the dual).

\textbf{Solution.}

\section{Question 5}
Given the following optimization problem 
\begin{align*}
    \min_{x \in \mathbb R, y > 0} \quad & e^{-x} \\
    \text{s.t.} \quad & \frac{x^2}{y} \leq 0
\end{align*}
\begin{enumerate}
    \item Show that this is a convex optimization problem, find its global minimizer, and verify if Slater condition holds.
    
    \textbf{Solution.}
    First, $f(x) = e^{-x}$ is a convex function since $$f(\lambda x + (1-\lambda)y) = e^{-\lambda x - (1-\lambda)y} \leq \lambda e^{-x} + (1-\lambda)e^{-y} = \lambda f(x) + (1-\lambda)f(y)$$
    Then, the feasible region is $$\Omega = \left\{(x, y): \frac{x^2}{y} \leq 0, y > 0\right\}$$ Since $x^2 \geq 0$ and $y > 0$, the only way that $\frac{x^2}{y} \leq 0$ is when $x = 0$. So the feasible region is $$\Omega = \{(0, y): y > 0\}$$ This is a convex set since $$\lambda(0, y_1) + (1-\lambda)(0, y_2) = (0, \lambda y_1 + (1-\lambda)y_2)$$ Since $y_1, y_2 > 0$, we get $\lambda y_1 + (1-\lambda)y_2 > 0$. So $(0, \lambda y_1 + (1-\lambda)y_2) \in \Omega$. So this is a convex optimization problem.

    \bigskip Now we find its global minimizer. Since the feasible region is $\Omega = \{(0, y): y > 0\}$, the global minimizer is $(0, y)$ for any $y > 0$ (the objective function does not depend on $y$). For a point to satisfy the Slater condition, we need $\frac{\bar x^2}{\bar y} \leq 0$, however, $\bar x = 0$, so $\frac{\bar x^2}{\bar y} = 0$. So the Slater condition does not hold.
    \item Find the dual problem of this problem; find the optimal solution of the dual problem; compute the duality gap.
    
    \textbf{Solution.}
    The Lagrangian is $$L(x, y, \lambda) = e^{-x} + \lambda\left(\frac{x^2}{y}\right)$$ The dual function is 
    \begin{align*}
        q(\lambda) &= \min_{x \in \mathbb R, y > 0} L(x, y, \lambda) \\
        &= \min_{x \in \mathbb R, y > 0} e^{-x} + \lambda\left(\frac{x^2}{y}\right) \\
        &= 1 \tag*{since $x$ is always 0}  
    \end{align*}
    So the dual problem is 
    $$\max_{\lambda \geq 0} \quad 1$$
    The optimal solution for the dual is any $\lambda \geq 0$. The optimal value is 1 for the dual. The optimal value is $e^{0} = 1$ for the primal. So the duality gap is $1 - 1 = 0$.
\end{enumerate}
\end{document}
